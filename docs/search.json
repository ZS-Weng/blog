[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaishan Weng",
    "section": "",
    "text": "I am techno-functional consultant and solutions architect with over 14 years of experience leading strategic digital transformation, data analytics and data science initiatives in the retail, government, health & security and restaurant industries. A dynamic & passionate professional who specializes in driving innovation, delivering large and complex projects, and advising senior executives on leveraging technology to drive business outcomes. A data scientist/engineer with proven track record in deploying impactful and productive AI products.\nI am passionate about the impact and difference technology has made and strive to explore ways of making existing work simpler and easier so that we can have the time to focus on bigger visions.\n\n\nCheck out my blog post on some of the Data Engineering, Data Science and Data Visualization learnings from implementing them in various projects here"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Zaishan’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nHandle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas\n\n\n\n\n\n\n\nPython\n\n\nData Cleaning\n\n\nExcel\n\n\nCSV\n\n\nData Engineering\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nSome ideas on enhancing User Experience on Qlik Sense\n\n\n\n\n\n\n\nQlik\n\n\nUser Experience\n\n\nBusiness Intelligence\n\n\nUser Interface\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nQuick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects\n\n\n\n\n\n\n\nDesign Thinking\n\n\nRequirements Gathering\n\n\nData Analytics\n\n\nStakeholder Engagement\n\n\nBusiness Intelligence\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nData cleaning on SAP data extracts in .txt format with Regex and Python\n\n\n\n\n\n\n\nData Engineering\n\n\nSAP\n\n\nPython\n\n\nRegex\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "",
    "text": "During one of our recent projects involving the procure to pay process, our team encountered SAP raw data extracted from the system in .txt format which proved to be difficult to clean using traditional methods like readlines() or split() by delimiters due to some inherent data inconsistencies.\nRegex matching proved to be helpful for such scenarios to clean the data."
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#common-sap-tables-in-procure-to-pay-process",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#common-sap-tables-in-procure-to-pay-process",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Common SAP tables in Procure to Pay process",
    "text": "Common SAP tables in Procure to Pay process\nFirst, lets go through what are common data tables extracted from SAP as part of the Procure to Pay process.\nAccounting Related Tables\n\nBKPF: Accounting Document Header\nBSAK: Accounting: Secondary Index for Vendors\nBSEG: Accounting Document Segment\n\nPurchase Order related Tables\n\nEKKO: Purchasing Document Header\nEKPO: Purchasing Document Item\nEKBE: History per Purchasing Document\nEKKN: Account Assignment in Purchasing Document\n\nMaterial Tables\n\nMAKT: Material Descriptions\n\nThere are various websites which provide additional information about the SAP tables.\n\nhttps://www.se80.co.uk/training-education/sap-tables/\nhttps://www.tcodesearch.com/sap-tables/detail?id=BSEG (Might need to enter through changing the id in URL without requiring premium membership)\n\n\n\n\nExample of Data Dictionary for BSEG Table Source: https://www.se80.co.uk/"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#format-of-sap-data-extract-in-.txt-file",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#format-of-sap-data-extract-in-.txt-file",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Format of SAP data extract in .txt file",
    "text": "Format of SAP data extract in .txt file\nFor our project, the output SAP data extracts is in a .txt format and with the typical structure as shown below:\n\nThe column header details starts at line 4\nThe width of each column is consistent between the column headers and the data for each file extracted\nActual data content starts at line 5 till the end\n\n\n\n\nSample SAP TXT Data Extract Structure with Mock Data\n\n\nThe sample SAP data in txt format and Jupyter Notebook can be found on GitHub: https://github.com/ZS-Weng/Data_Engineering/tree/main/Data_Cleaning\nThe two major data discrepancies encountered are:\n\nNewline character inserted in some of the fields\nPipe (|) delimiters found within the actual data"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#full-code-and-output",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#full-code-and-output",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Full Code and Output",
    "text": "Full Code and Output\nThe full working code for the data cleaning is as shown below:\nimport pandas as pd\nimport re\n\n# Read File\nwith open(\"Sample SAP Format.txt\", encoding=\"utf-8\") as f:\n    content_raw = f.read()\n\n# Clean extra newline characters\nnew_line_pattern = re.compile(\"([^1|-])[\\n](.)|(.)[\\n]([^|-])\")\ncontent_cleaned_newline = new_line_pattern.sub(r\"\\1 \\2\", content_raw)\ncontent_split_line = content_cleaned_newline.split(\"\\n\")\n\n# Clean the rest of content\n\n# Extract Header and Row Pattern\nheader_string = content_split_line[3]\ncolumn_header = [token.strip() for token in header_string.split(\"|\")][1:-1]\nlist_column_width = [\n    \"(.{\" + str(len(column)) + \"})\" for column in header_string.split(\"|\")\n][1:-1]\ncolumn_string_pattern = \"[|]\" + \"[|]\".join(list_column_width) + \"[|]\"\n#Extract Data Body\ncolumn_pattern = re.compile(column_string_pattern)\ncleaned_content = [\n    [token.strip() for token in column_pattern.match(row).groups()]\n    for row in content_split_line[5:-2]\n]\ndf_clean = pd.DataFrame(cleaned_content, columns=column_header)\nFinal pandas data frame output:\n\n\n\nFinal pandas tabular output after data cleaning"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#detailed-walk-through-of-the-codes",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#detailed-walk-through-of-the-codes",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Detailed Walk-through of the codes",
    "text": "Detailed Walk-through of the codes\nCleaning of newline character prior to splitting lines\nIn order to clean the (newline) characters which are not valid, we first use the read() instead of readline() method as the readline() method will split the lines by the character automatically.\nwith open(\"Sample SAP Format.txt\", encoding=\"utf-8\") as f:        \n    content_raw = f.read()\nNext, we use the Regex pattern ([1|-])[\\n](.)|(.)[\\n]([^|-]) to find and the invalid characters. The pattern basically detects newline that do not start with the characters 1 , | or - and do not end with the characters | or -.\nFor the invalid newline characters found, we replace them with a space using the .sub() method.\nnew_line_pattern = re.compile(\"([^1|-])[\\n](.)|(.)[\\n([^|-])\")\ncontent_cleaned_newline = new_line_pattern.sub(r\"\\1 \\2\",content_raw)\ncontent_split_line = content_cleaned_newline.split(\"\\n\")\nOutput after cleaning and splitting lines:\n\n\n\nList of Data After the Line Split\n\n\nData Cleaning Steps for Individual Line Based on the above, we can see that the data we are interested in is the column header (4th line) and the rest of the data content (6th to 2nd last lines).\nWe will first split the column by the pipe “|” delimiter and getting the columns width to create the Regex matching string pattern.\nheader_string = content_split_line[3]\ncolumn_header = [token.strip() for token in header_string.split(\"|\")][1:-1]\nlist_column_width = [\"(.{\" + str(len(column)) + \"})\" for column in header_string.split(\"|\")][1:-1]\ncolumn_string_pattern = \"[|]\" + \"[|]\".join(list_column_width) + \"[|]\"\nThe column_string_pattern to match that is generated for the sample data will be as follows:\n[|](.{10})[|](.{4})[|](.{3})[|](.{10})[|](.{20})[|]\nThis column pattern matching pattern is dynamically generated and will be unique for each of the data extract file even for the same tables as the width is adjusted during data extract according to the content.\nWe will then use the matching pattern to extract the rest of the data (6th to 2nd last line) with re.match().groups() instead of using the str.split() method.\ncolumn_pattern = re.compile(column_string_pattern)\ncleaned_content = [[token.strip() for token in column_pattern.match(row).groups()] for row in content_split_line[5:-2]]\nThe output after content splitting:\n\n\n\nData in Nested List format after splitting of content of each individual line\n\n\nHere we can see that the extra delimiters e.g. ‘Item A|B|C’ does not affect the content splitting .\nFinally, we combine the cleaned header columns and data into a pandas data frame.\ndf_clean = pd.DataFrame(cleaned_content, columns=column_header)\nFinal pandas Data Frame output:\n\n\n\nFinal cleaned data output in pandas Data Frame"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#useful-learning-resource-for-python-and-regex",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#useful-learning-resource-for-python-and-regex",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Useful Learning Resource for Python and Regex",
    "text": "Useful Learning Resource for Python and Regex\nWhen I was starting out, I found the book Automate the Boring Stuff with Python by Al Sweigart one of the best resources to learning about Python and Regex with many practical examples.\nThere is a free access option to the book on his website: https://automatetheboringstuff.com/ and this is the link to the specific chapter on Regex which formed the foundation on some of the implementation: https://automatetheboringstuff.com/2e/chapter7/\nThanks for reading and hope the information was useful in some way!"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "",
    "text": "After the successful delivery of a data analytics product, a common challenge faced is to enhance user adoption. One of the definitions of user adoption from an Indeed article is as follows:\nUser adoption rates are important because they tell a company or business how many users like the new product or version and how many don’t like it or do not try it. Most often, a higher adoption rate means a customer finds both value and ease in using the new product or version. Conversely, if a customer finds it requires too much effort to use or doesn’t add value, they may abandon the product altogether or stay with an older version.\nhttps://www.indeed.com/career-advice/career-development/user-adoption\nOne of the necessary ingredients to a successful adoption is the user experience on the interface. In this article, I would like to cover on ways we have found useful to make dashboards on Qlik Sense Apps more intuitive and user friendly.\nThe three areas I would like to cover are as follows:\n\nMenu Page for holistic view of overall health of business\n\n\n\n\nExample of Menu Page with KPIs on Qlik Sense\n\n\n\nNavigation Bars for navigating between apps and within each app\n\n\n\n\nNavigation between App and Within App\n\n\n\nOnline Help and Information\n\n\n\n\nHelp Page Built In Online with the App\n\n\nThe Qlik Sense demo app can be found in the following Github link:\nhttps://github.com/ZS-Weng/QlikSense/blob/main/QlikSense%20Navigation.qvf"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "",
    "text": "From my recent involvement in data analytics project engagements as the Data Science and Analytics Lead at IBM, it was observed that proper requirements gathering in the project initiation phase can make a big difference to the eventual success of the project.\nOne of the common challenges faced during this phase revolves around translating high level management visions at 50,000ft (e.g. Embark on Digital Transformation, Employ AI) into definitive and objective project requirements for implementation. From my experience, the use of appropriate design thinking artifacts can help to guide the process more effectively.\nIn this article, I would like to share a high level overview on the the design thinking artifacts which worked well on our projects and an example of how they are typically woven together during the project initiation phase. Most of these artifacts shared are aligned to IBM Design Thinking Methodology and additional details are available on the IBM Design Thinking website. https://www.ibm.com/design/thinking/ (signing up for a free IBM account might be required to access the materials)"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#breaking-down-the-initial-requirements-gathering-process",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#breaking-down-the-initial-requirements-gathering-process",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "Breaking down the Initial Requirements Gathering Process",
    "text": "Breaking down the Initial Requirements Gathering Process\nTypically the initial requirements gathering process can be broken down three main steps starting with crystallizing the overall business case of the project to the generation of project requirements. We will go through more details in each of the main steps:\n\nStep 1- Defining goals and objectives of the project\nStep 2- Understanding users, their current work process and envisioned to-be state\nStep 3- Initial formulation of analytics solution requirements\n\n\nStep 1: Defining Goals and Objectives of the Project\nThe first step usually involves connecting with the project sponsor and key stakeholders who will be responsible for the outcome of the project. It is critical for the stakeholders and project team to come together and define the overall direction of the project.\nTypical Stakeholders from Client: Project Sponsor, Product Owner, Project Manager, SMEs, Users\nKey Objectives:\n\nEstablish overall business case for the project including potential benefits and how success of the project will be measured\nIdentify and understand key stakeholders on the project\nIdentify risks and potential road blocks\n\nKey Design Thinking Artifacts:\n\nHopes and Fears\nOpportunity Canvas\nStakeholder Map\nStakeholder Matrix\nAssumptions & Questions\n\n\nHopes and Fears\nThe Hopes and Fears artifact is useful as a warm up exercise to gather inputs from the stakeholders about their hopes for the project and gain insights to their worries and concerns. At times, the fears section might also help to uncover potential pitfalls and lessons learnt from previous projects.\n\n\n\nSample of a completed Hopes & Fears artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/hopes-and-fears\n\n\n\n\nOpportunity Canvas\nOpportunity Canvas is a very helpful artifact to provide holistic view of a project. The canvas can provide assessment on the overall readiness and identify areas with potential gaps and challenges. It is a working document that will be constantly refined throughout the requirements gathering phase. \nIn our engagements, we have further expanded and customized the sections of the opportunity canvas:\n\nProblems/Goals to be Solved\nSolution Ideas\nValue Proposition\nKey Metrics\nTarget Customer/User Type Segments\nHigh Level Business Benefits and Impact\nCompetitive Landscape\nDistribution Channel/Adoption Strategy\nKey Partners/Alliances\nCost Structure\nAssumptions/Questions\nConstraints\nImpediments\nSquad Members Required\nBU Support Required\nAssets & Accelerators\n\n\n\nStakeholder Map & Stakeholder Matrix\nA better understanding of stakeholders involved is critical to garner support for the project and ensure all relevant stakeholders are sufficiently engaged. The first part of the exercise will involve listing down all potential stakeholders, grouping them and identifying the relationships.\n\n\n\nSample Stakeholder Mapping from https://www.ibm.com/design/thinking/page/toolkit/activity/stakeholder-map\n\n\nIn addition to the stakeholder map, the stakeholder matrix provides another useful perspective by locating each stakeholder in the 2 by 2 matrix of Influence vs Interest.\nThe stakeholder map and matrix can also help to customize the communications and engagement plan for the various stakeholder groups. (e.g. type of updates, frequency of updates, attendance at meetings etc.)\n\n\n\n2 X 2 Stakeholder Matrix of Influence vs Interest\n\n\n\n\nAssumptions and Questions\nThe Assumptions and Questions chart can be prefilled with relevant information throughout the earlier discussions. At this stage, the chart can help to recap on existing assumptions, add new details that were missed and perform an initial assessment on the impact and probability as a group.\n\n\n\nSample Assumptions and Questions artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/assumptions-and-questions\n\n\n\n\n\nStep 2: Understanding users, their current work process and pain points\nWith an alignment on the overall business case and strategic direction of the project, we will shift gears in Step 2 to understand more about the end users and details around their work processes in the current and future context.\nTypical Stakeholders from Client: SMEs and Users from the different groups who will be the end users\nKey Objectives:\n\nUnderstanding the end users, their current work process and how they are expected to interact with the solution in their work process\nIdentify wish lists and pain points\n\nKey Design Thinking Artifacts:\n\nEmpathy Map\nTo-Be Scenario Map\n\n\nEmpathy Map\nThe empathy map aids in the understanding of the profile for the users. It is best practice to generate empathy maps for each group of users. The collection of details can be either in the context of the problem statement or in the wider context of their day to day work. The details of what they say, do, think and feel will help to paint a representative user profile of the group.\n\n\n\nSample Empathy Map artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/empathy-map\n\n\n\n\nTo-Be Scenario Map\nBuilding upon the empathy map for each user group, additional details on each phase of the to-be working process can be consolidated to form the To-Be scenario map. The discussion with the users in the dimensions of doing, thinking and feeling for each of the phases will help to uncover pain points, gaps and potential benefits.\n\n\n\nSample To-Be Scenario Map artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/to-be-scenario-map\n\n\n\n\n\nStep 3- Initial formulation of analytics solution requirements\nIn Step 3, we will build upon the established direction and understanding of users and their needs & wants. The key goal in this phase is to develop the solution requirements in sufficient details to start the project.\nTypical Stakeholders from Client: SMEs and Users from the different groups who will be the end users\nKey Objectives:\n\nDeveloping high level requirements in hills or user stories format\nBrainstorm on functional features\nPrioritize features for development\n\nKey Design Thinking Artifacts: * Hills (User Stories) * Big Idea * Prioritization Map\n\nUser Stories or Hills Writing (Who, What, Wow)\nThe gathering of User Stories or Hills captures user requirements the following format:\n\nWho (In the capacity of a certain Role),\nWhat (I would like to take some action),\nWow (To achieve a certain outcome).\n\nThis will be useful to identify target users, identify user interactions with the solution and understand the underlying purpose and outcome to be achieved.\n\n\n\nSample Hills Writing Exercise output from https://www.ibm.com/design/thinking/page/toolkit/activity/writing-hills\n\n\n\n\nBig Idea\nBased on the user stories & hills captured, we can conduct a brainstorming session to draw out ideas on features required in each of the user stories & hills. Similar features can be categorized and grouped together to form a coherent functionality set.\n\n\n\nSample Big Idea Collections from https://www.ibm.com/design/thinking/page/toolkit/activity/big-idea-vignettes\n\n\n\n\nPrioritization Map\nThe next step involves placing the features and functionality on the prioritization map. Features in the No Brainers section with the highest impact and feasibility will be planned with the highest priority for development first. This will help the project to deliver benefits early on. There is usually some deliberation involve in the prioritization between the big bets section (High value but might be less feasible) and utilities section (Higher feasibility but might deliver less value). The features in the un-wise section with low impact and feasibility will have least lower priority and are usually parked for future review.\n\n\n\nSample Prioritization Grid from https://www.ibm.com/design/thinking/page/toolkit/activity/prioritization"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#transiting-from-project-initiation-phase-to-project-planning-phase",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#transiting-from-project-initiation-phase-to-project-planning-phase",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "Transiting from Project Initiation Phase to Project Planning Phase",
    "text": "Transiting from Project Initiation Phase to Project Planning Phase\nA logical transition point from the project initiation phase to the project planning phase will be the step where the details from the prioritization map is converted into the product backlog.\n\nProduct Backlog Hierarchy\nA typical product backlog can consist of various layers depending on the complexity of the project and terminology that the team or existing tool follows. Using an example from Azure DevOps board, a typical structure on the product backlog will be: * Epic * Feature * User Story (Product Backlog Item) * Task\n\n\n\nAzure Dev Ops Board Standard Scrum Backlog Structure\n\n\nUsing the inputs from the prioritization map, the team will work closely with the product owner and users to develop the product backlog. Based on experience, most of the information captured will be at the level of features and product backlog items. The features can be grouped into Epics signifying features for each product release and broken down into product backlog items.\nThe rest of the project planning activities will follow on from this phase.\n\n\nExample of Product Backlog Items for Data Visualization Dashboard\nFor data visualization features, there are three standard components to be captured as part of the product backlog as shown below:\n\n\n\nStandard Components of a Data Visualization Workflow\n\n\nFor most dashboard visualization requirements on BI (Business Intelligence) tools such as Tableau, Qlik, PowerBI, the standard functional requirements that needs to be defined are as follows:\nAnalytics Flow: The usual analytics process usually presents information in a top-down approach, moving from a big picture summary to detailed report with many rows of data. An example of this is Qlik’s DAR(Dashboard, Analysis, Reporting) methodology where information in presented from the highest granularity to the lowest granularity.\nDimensions: Identifying the dimensions /features is a critical component of the function visualization requirements. From the identified dimensions, the lowest granularity of the data and a hierarchy can be derived. In addition, selected dimensions can be defined as filters. A subset of commonly used dimensions and their hierarchies in a retail context is shown below:\n\n\n\nSample of common dimensions in the retail context\n\n\nKPIs: KPIs are any of the columns that can provide a quantitative measure against the dimensions. It is important to capture the business rules and transformations clearly and ensure that it is aligned across the different group of stakeholders. A subset of commonly used metrics in a retail context is as shown below:\n\n\n\nSample of commonly used KPIs in the retail context\n\n\nThanks for reading and hope the information was useful in some way!"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#menu-page-for-holistic-view-of-overall-health-of-business",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#menu-page-for-holistic-view-of-overall-health-of-business",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "Menu Page for holistic view of overall health of business",
    "text": "Menu Page for holistic view of overall health of business\nIn a data analytics / business intelligence product, it usually can consist of several apps or reports based on different aspects of the business. Inspired by Fitbit’s health analysis charts, the menu page for a Qlik Sense dashboard is customized with icons and the most important KPI. Based on the thresholds set, users can have an holistic understanding on the overall health summary from various aspects of the business.\nThe picture below illustrates how the color of the KPI and icon will change based on the changing value. The threshold set is 33 and 66.\n\n\n\nMenu Page showing how the color of the KPI and Icons will change based on the KPI value and threshold\n\n\nQlik Sense Codes: The thresholding can easily be setup in Qlik Sense with if statements.\n\nCode for controlling the colors on the KPI and Icons in Qlik Sense:\n\n=if(v_KPI3 <= 33, ‘#e8291c’,\nif( v_KPI3 > 33 and v_KPI3 <= 66 , ‘#abaa20’, \nif( v_KPI3 > 66, ‘#25af2f’)))\n\nCode for controlling the Icon Image in Qlik Sense:\n\n=if(v_KPI3 <= 33, ‘Warning triangle’,\nif( v_KPI3 > 33 and v_KPI3 <= 66 , ‘View’, \nif( v_KPI3 > 66, ‘Tick’)))\nOther resources for creating the icons\n\nFree icons can be downloaded from: https://icons8.com/\nThe websites that provides reference on beautiful gradient color combinations: https://uigradients.com/ , https://digitalsynopsis.com/design/beautiful-color-ui-gradients-backgrounds/\nOnline tools that can invert a black and white icon: https://www.google.com/search?q=image+invert\nPython Code that can invert a black and white icon:\n\nimport os\nfrom PIL import Image, ImageOps\nfrom matplotlib.pyplot import imshow\nfrom pathlib import Path\n\n\"\"\"\nSetup Base and Process Folder\n\"\"\"\n# Enter base folder path\nbase_folder = Path(\"./icons\")\nimage_paths = Path.glob(base_folder, \"*.png\")\nprocessed_folder = \"./icons/processed/\"\nos.makedirs(processed_folder, exist_ok=True)\n\n\ndef image_invert(image_path):\n    \"\"\"\n    Perform image inversion of the colots and set transparent map\n    for black pixels\n    \"\"\"\n    image = Image.open(image_path)\n    if image.mode == \"RGBA\":\n        r, g, b, a = image.split()\n        rgb_image = Image.merge(\"RGB\", (r, g, b))\n        inverted_image = ImageOps.invert(rgb_image)\n        r2, g2, b2 = inverted_image.split()\n        final_transparent_image = Image.merge(\"RGBA\", (r2, g2, b2, a))\n        return final_transparent_image\n\n    else:\n        inverted_image = ImageOps.invert(image)\n        return inverted_image\n\n\nfor image_path in image_paths:\n    inverted_image = image_invert(image_path)\n    newpath = Path.joinpath(base_folder, \"processed\", image_path.stem + \"_inverted.png\")\n    inverted_image.save(newpath)"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#navigation-bars-between-apps-and-within-each-app",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#navigation-bars-between-apps-and-within-each-app",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "Navigation Bars between apps and within each app",
    "text": "Navigation Bars between apps and within each app\nTo make the navigation across apps and within an app more intuitive, buttons can be used to create the respective navigations. In our projects, we aim to create an experience similar to navigating websites with the side bars providing navigation between apps and top bars within each app. The top bars for our case are standardized to enable users to drill down from Summary > Analysis > Details.\n\n\n\nExample of Navigation between the Apps\n\n\nSample Code in Qlik Sense on Variable Setup:\nSet v_TopBarSelectedColor = ‘#8351a8’;\nSet v_TopBarUnSelectedColor = ‘#4d7b93’;\nSet v_SideBarSelectedColor = ‘#8351a8’;\nSet v_SideBarUnSelectedColor = ‘#4d7b93’;\nSet v_SideBar1Text = ‘App 1’;\nSet v_SideBar2Text = ‘App 2’;\nSet v_SideBar3Text = ‘App 3’;\nSet v_SideBar4Text = ‘Help Page’;\nSet v_SideBar1Link = ‘a0d70b49–7a21–44d0-b9a9-daaa0701c175’;\nSet v_SideBar2Link = ‘e03b4a67–89c1–425b-852c-44d112e48d7c’;\nSet v_SideBar3Link = ‘adc68de9–5a42–40ef-bdcf-dffa2981d83c’;\nSet v_SideBar4Link = ‘f3e454e2-ea27–46ef-bc5a-d1ddd280aaf9’;\nSet v_helppageLink = ‘f3e454e2-ea27–46ef-bc5a-d1ddd280aaf9’;\nSet v_menupagelink = ‘be0a8400–5df7–4783–8339–1e44e66b04e5’;"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#online-help-and-information",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#online-help-and-information",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "Online Help and Information",
    "text": "Online Help and Information\nIn many projects, often training materials are shared in PowerPoint decks or word documents. These documents might not be easily accessible to every user and it may become challenging at times for users to find the right reference guide when they need help while navigating an analytics app. In our projects, we recommend for the help pages to be built in within as an accompanying sheet or app so that it will be online together with the analytics product.\n\n\n\nNavigation to Help Page and use of Containers as Step by Step Guide"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#conclusion",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#conclusion",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, I have aimed to share how user experience can be enhanced on Qlik Sense applications with out of the box capabilities.\nAny further enhancements will likely require tools beyond the application builder like Mashups or D3.js custom visualizations.\nThanks for reading and hope the information was useful in some way!"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "",
    "text": "Image generated from Hugging Face Stable Diffusion with the prompt “multitude of sources coming together in a mural”"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#introduction",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#introduction",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "Introduction",
    "text": "Introduction\nIn the process of ingesting and cleaning raw data in various formats using python, there were some tricks found to be useful based on the issues which were encountered. Types of source files covered in the article so far are as follows:\n\nxlsb (excel binary format)\ncsv files where encoding are not UTF-8\nexcel files with multiple tabs\nraw data files in .txt format\nparquet files\n\nThe corresponding notebook and sample data can be found at https://github.com/ZS-Weng/Data_Engineering/tree/main/Data_Cleaning"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#reading-in-excel-files-in-.xlsb-format",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#reading-in-excel-files-in-.xlsb-format",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "1. Reading in excel files in .xlsb format",
    "text": "1. Reading in excel files in .xlsb format\nThere are various formats of excel files e.g. .xls, .xlsx, .xlsb, .xlsm etc. Based on the out of the box functionality from Pandas, the formats .xlsx and .xls are supported and the .xlsb (binary) format is not supported out of the box.\nThe most efficient solution I found is to install the pyxlsb extension. More details can be found at the PyPI project website: https://pypi.org/project/pyxlsb/\nThe code to execute including installation of the library will be as follows:\npip install pyxlsb\ndf = pd.read_excel(file, engine=”pyxlsb”)"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#resolve-unicodedecodeerror-during-csv-file-read",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#resolve-unicodedecodeerror-during-csv-file-read",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "2. Resolve UnicodeDecodeError during CSV file read",
    "text": "2. Resolve UnicodeDecodeError during CSV file read\nMost of the files are encoded in “UTF-8” which is the default encoding. However, there are times when when the encoding in not “UTF-8” and there will be an error message similar to this:\nUnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xff in position 0: invalid start byte\nThe most effective method I have encountered is to first read the file as binary using ‘rb’ mode to determine the encoding, then apply the encoding which was detected.\nimport chardet\nwith open(file, 'rb') as f:\n    encoding = chardet.detect(f.read())\nprint(encoding)\nThe result will be something like this:\n\n\n\nDetecting encoding in a file\n\n\nBased on the encoding identified, we can use the encoding in pd.read_csv()\ndf = pd.read_csv(file, encoding=encoding['encoding'])\nThe result will look something like this:\n\n\n\nReading the csv file based on updated encoding"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#extracting-data-from-multiple-sheets-in-excel",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#extracting-data-from-multiple-sheets-in-excel",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "3. Extracting data from multiple sheets in excel",
    "text": "3. Extracting data from multiple sheets in excel\nThere are times where the data in excel might come from multiple sheets and we need to read in the data from all the sheets. The most straightforward way which I have found is using pd.read_excel with the parameter sheet_name=None. This will read all of the sheets in excel into a dictionary with the sheet name as the key and the data in DataFrames tagged to each sheet name.\nHere we use an example where a sample SAP data dictionary with the relevant data schema. The details for each table are shared in each sheet while each sheet in excel represents the respective tables.\n\n\n\nSample SAP Data Dictionary shared via Excel\n\n\nWe can read in the data in all the sheets with the parameter sheet_name set to None. Here instead of a single data frame, we will have a dictionary with each sheet name as key and the data frame in the sheet as the corresponding value of the dictionary.\n# Reading in the data\nsample_SAP_data_dict = \"./data/SAP Data Dictionary Selected Tables.xlsx\"\ndata_sheet_dict = pd.read_excel(sample_SAP_data_dict, sheet_name=None)\n# Display the names of each sheet in excel\nlist_sheets = list(data_sheet_dict.keys())\nprint(f\"There are {len(list_sheets)} sheets in the excel file\")\nprint(f\"The sheets are {list_sheets}\")\nAs we print out the keys, we can see the sheet names being listed. In our example, this corresponds with some of the SAP table names.\n\n\n\nDescription of excel file\n\n\nBelow is an example when the key “BSEG” is applied for the dictionary, we can get the corresponding data frame which was read from the sheet containing the schema of the table.\n\n\n\nSample Data Frame for BSEG table\n\n\nSubsequent analysis can be run at scale using the normal python dictionary and pandas functions and methods.\nOne of the analysis performed for our sample Data Dictionary is to find out which tables houses the similar data fields. We will first use a pivot_table method to create a new table where the column fields are shown as columns for each table. From there we join the tables together using pd.concat.\ndef extract_columns_info(df, sheet_name, list_column_field):\n    df_temp = df.copy()\n    df_temp[\"SAP_Table\"] = sheet_name\n    df_temp[\"Present\"] = 1\n    columns_select = [\"SAP_Table\"] + list_column_field + [\"Present\"]\n    \n    return df_temp[columns_select].pivot_table(\n        columns=list_column_field, values=\"Present\", index=[\"SAP_Table\"]\n    )\nlist_column_field = [\"Field\"]\nlist_df = []\nfor sheet_name, df in data_sheet_dict.items():\n    df_processed = extract_columns_info(df, sheet_name, list_column_field)\n    list_df.append(df_processed)\n\ndf_consolidated = pd.concat(list_df)\ndf_consolidated.fillna(\"\", inplace=True)\nIn the final result, we can we can see the common fields across various SAP tables. Below is a sample of the output.\n\n\n\nSample output of final table displaying common fields across different tables"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#reading-.txt-source-files",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#reading-.txt-source-files",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "4. Reading .txt source files",
    "text": "4. Reading .txt source files\nFor input data sources in .txt format, we can use the python open function directly to read the files. There will usually be text processing required depending on the delimiters used in the files.\nI wrote a seperate article for the cleaning of SAP .txt input file format. The article can be found here\nThe .read() method can be used to read in the entire data source. There are usually some processing involved e.g. all types of new line characters are represented with ."
  }
]