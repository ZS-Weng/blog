[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaishan Weng",
    "section": "",
    "text": "I am techno-functional consultant and solutions architect with over 14 years of experience leading strategic digital transformation, data analytics and data science initiatives in the retail, government, health & security and restaurant industries. A dynamic & passionate professional who specializes in driving innovation, delivering large and complex projects, and advising senior executives on leveraging technology to drive business outcomes. A data scientist/engineer with proven track record in deploying impactful and productive AI products.\nI am passionate about the impact and difference technology has made and strive to explore ways of making existing work simpler and easier so that we can have the time to focus on bigger visions."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Zaishan’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nSome ideas on enhancing User Experience on Qlik Sense\n\n\n\n\n\n\n\nDesign Thinking\n\n\nRequirements Gathering\n\n\nData Analytics\n\n\nStakeholder Engagement\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nQuick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects\n\n\n\n\n\n\n\nDesign Thinking\n\n\nRequirements Gathering\n\n\nData Analytics\n\n\nStakeholder Engagement\n\n\nBusiness Intelligence\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nData cleaning on SAP data extracts in .txt format with Regex and Python\n\n\n\n\n\n\n\nData Engineering\n\n\nSAP\n\n\nPython\n\n\nRegex\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "",
    "text": "During one of our recent projects involving the procure to pay process, our team encountered SAP raw data extracted from the system in .txt format which proved to be difficult to clean using traditional methods like readlines() or split() by delimiters due to some inherent data inconsistencies.\nRegex matching proved to be helpful for such scenarios to clean the data."
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#common-sap-tables-in-procure-to-pay-process",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#common-sap-tables-in-procure-to-pay-process",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Common SAP tables in Procure to Pay process",
    "text": "Common SAP tables in Procure to Pay process\nFirst, lets go through what are common data tables extracted from SAP as part of the Procure to Pay process.\nAccounting Related Tables\n\nBKPF: Accounting Document Header\nBSAK: Accounting: Secondary Index for Vendors\nBSEG: Accounting Document Segment\n\nPurchase Order related Tables\n\nEKKO: Purchasing Document Header\nEKPO: Purchasing Document Item\nEKBE: History per Purchasing Document\nEKKN: Account Assignment in Purchasing Document\n\nMaterial Tables\n\nMAKT: Material Descriptions\n\nThere are various websites which provide additional information about the SAP tables.\n\nhttps://www.se80.co.uk/training-education/sap-tables/\nhttps://www.tcodesearch.com/sap-tables/detail?id=BSEG (Might need to enter through changing the id in URL without requiring premium membership)\n\n\n\n\nExample of Data Dictionary for BSEG Table Source: https://www.se80.co.uk/"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#format-of-sap-data-extract-in-.txt-file",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#format-of-sap-data-extract-in-.txt-file",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Format of SAP data extract in .txt file",
    "text": "Format of SAP data extract in .txt file\nFor our project, the output SAP data extracts is in a .txt format and with the typical structure as shown below:\n\nThe column header details starts at line 4\nThe width of each column is consistent between the column headers and the data for each file extracted\nActual data content starts at line 5 till the end\n\n\n\n\nSample SAP TXT Data Extract Structure with Mock Data\n\n\nThe sample SAP data in txt format and Jupyter Notebook can be found on GitHub: https://github.com/ZS-Weng/Data_Engineering/tree/main/Data_Cleaning\nThe two major data discrepancies encountered are:\n\nNewline character inserted in some of the fields\nPipe (|) delimiters found within the actual data"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#full-code-and-output",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#full-code-and-output",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Full Code and Output",
    "text": "Full Code and Output\nThe full working code for the data cleaning is as shown below:\nimport pandas as pd\nimport re\n\n# Read File\nwith open(\"Sample SAP Format.txt\", encoding=\"utf-8\") as f:\n    content_raw = f.read()\n\n# Clean extra newline characters\nnew_line_pattern = re.compile(\"([^1|-])[\\n](.)|(.)[\\n]([^|-])\")\ncontent_cleaned_newline = new_line_pattern.sub(r\"\\1 \\2\", content_raw)\ncontent_split_line = content_cleaned_newline.split(\"\\n\")\n\n# Clean the rest of content\n\n# Extract Header and Row Pattern\nheader_string = content_split_line[3]\ncolumn_header = [token.strip() for token in header_string.split(\"|\")][1:-1]\nlist_column_width = [\n    \"(.{\" + str(len(column)) + \"})\" for column in header_string.split(\"|\")\n][1:-1]\ncolumn_string_pattern = \"[|]\" + \"[|]\".join(list_column_width) + \"[|]\"\n#Extract Data Body\ncolumn_pattern = re.compile(column_string_pattern)\ncleaned_content = [\n    [token.strip() for token in column_pattern.match(row).groups()]\n    for row in content_split_line[5:-2]\n]\ndf_clean = pd.DataFrame(cleaned_content, columns=column_header)\nFinal pandas data frame output:\n\n\n\nFinal pandas tabular output after data cleaning"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#detailed-walk-through-of-the-codes",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#detailed-walk-through-of-the-codes",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Detailed Walk-through of the codes",
    "text": "Detailed Walk-through of the codes\nCleaning of newline character prior to splitting lines\nIn order to clean the (newline) characters which are not valid, we first use the read() instead of readline() method as the readline() method will split the lines by the character automatically.\nwith open(\"Sample SAP Format.txt\", encoding=\"utf-8\") as f:        \n    content_raw = f.read()\nNext, we use the Regex pattern ([1|-])[\\n](.)|(.)[\\n]([^|-]) to find and the invalid characters. The pattern basically detects newline that do not start with the characters 1 , | or - and do not end with the characters | or -.\nFor the invalid newline characters found, we replace them with a space using the .sub() method.\nnew_line_pattern = re.compile(\"([^1|-])[\\n](.)|(.)[\\n([^|-])\")\ncontent_cleaned_newline = new_line_pattern.sub(r\"\\1 \\2\",content_raw)\ncontent_split_line = content_cleaned_newline.split(\"\\n\")\nOutput after cleaning and splitting lines:\n\n\n\nList of Data After the Line Split\n\n\nData Cleaning Steps for Individual Line Based on the above, we can see that the data we are interested in is the column header (4th line) and the rest of the data content (6th to 2nd last lines).\nWe will first split the column by the pipe “|” delimiter and getting the columns width to create the Regex matching string pattern.\nheader_string = content_split_line[3]\ncolumn_header = [token.strip() for token in header_string.split(\"|\")][1:-1]\nlist_column_width = [\"(.{\" + str(len(column)) + \"})\" for column in header_string.split(\"|\")][1:-1]\ncolumn_string_pattern = \"[|]\" + \"[|]\".join(list_column_width) + \"[|]\"\nThe column_string_pattern to match that is generated for the sample data will be as follows:\n[|](.{10})[|](.{4})[|](.{3})[|](.{10})[|](.{20})[|]\nThis column pattern matching pattern is dynamically generated and will be unique for each of the data extract file even for the same tables as the width is adjusted during data extract according to the content.\nWe will then use the matching pattern to extract the rest of the data (6th to 2nd last line) with re.match().groups() instead of using the str.split() method.\ncolumn_pattern = re.compile(column_string_pattern)\ncleaned_content = [[token.strip() for token in column_pattern.match(row).groups()] for row in content_split_line[5:-2]]\nThe output after content splitting:\n\n\n\nData in Nested List format after splitting of content of each individual line\n\n\nHere we can see that the extra delimiters e.g. ‘Item A|B|C’ does not affect the content splitting .\nFinally, we combine the cleaned header columns and data into a pandas data frame.\ndf_clean = pd.DataFrame(cleaned_content, columns=column_header)\nFinal pandas Data Frame output:\n\n\n\nFinal cleaned data output in pandas Data Frame"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#useful-learning-resource-for-python-and-regex",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#useful-learning-resource-for-python-and-regex",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Useful Learning Resource for Python and Regex",
    "text": "Useful Learning Resource for Python and Regex\nWhen I was starting out, I found the book Automate the Boring Stuff with Python by Al Sweigart one of the best resources to learning about Python and Regex with many practical examples.\nThere is a free access option to the book on his website: https://automatetheboringstuff.com/ and this is the link to the specific chapter on Regex which formed the foundation on some of the implementation: https://automatetheboringstuff.com/2e/chapter7/"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "",
    "text": "After the successful delivery of a data analytics product, a common challenge faced is to enhance user adoption. One of the definitions of user adoption from an Indeed article is as follows:\nUser adoption rates are important because they tell a company or business how many users like the new product or version and how many don’t like it or do not try it. Most often, a higher adoption rate means a customer finds both value and ease in using the new product or version. Conversely, if a customer finds it requires too much effort to use or doesn’t add value, they may abandon the product altogether or stay with an older version.\nhttps://www.indeed.com/career-advice/career-development/user-adoption\nOne of the necessary ingredients to a successful adoption is the user experience on the interface. In this article, I would like to cover on ways we have found useful to make dashboards on Qlik Sense Apps more intuitive and user friendly.\nThe three areas I would like to cover are as follows:\n\nMenu Page for holistic view of overall health of business\n\nNavigation Bars for navigating between apps and within each app\nOnline Help and Information"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "",
    "text": "From my recent involvement in data analytics project engagements as the Data Science and Analytics Lead at IBM, it was observed that proper requirements gathering in the project initiation phase can make a big difference to the eventual success of the project.\nOne of the common challenges faced during this phase revolves around translating high level management visions at 50,000ft (e.g. Embark on Digital Transformation, Employ AI) into definitive and objective project requirements for implementation. From my experience, the use of appropriate design thinking artifacts can help to guide the process more effectively.\nIn this article, I would like to share a high level overview on the the design thinking artifacts which worked well on our projects and an example of how they are typically woven together during the project initiation phase. Most of these artifacts shared are aligned to IBM Design Thinking Methodology and additional details are available on the IBM Design Thinking website. https://www.ibm.com/design/thinking/ (signing up for a free IBM account might be required to access the materials)"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#breaking-down-the-initial-requirements-gathering-process",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#breaking-down-the-initial-requirements-gathering-process",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "Breaking down the Initial Requirements Gathering Process",
    "text": "Breaking down the Initial Requirements Gathering Process\nTypically the initial requirements gathering process can be broken down three main steps starting with crystallizing the overall business case of the project to the generation of project requirements. We will go through more details in each of the main steps:\n\nStep 1- Defining goals and objectives of the project\nStep 2- Understanding users, their current work process and envisioned to-be state\nStep 3- Initial formulation of analytics solution requirements\n\n\nStep 1: Defining Goals and Objectives of the Project\nThe first step usually involves connecting with the project sponsor and key stakeholders who will be responsible for the outcome of the project. It is critical for the stakeholders and project team to come together and define the overall direction of the project.\nTypical Stakeholders from Client: Project Sponsor, Product Owner, Project Manager, SMEs, Users\nKey Objectives:\n\nEstablish overall business case for the project including potential benefits and how success of the project will be measured\nIdentify and understand key stakeholders on the project\nIdentify risks and potential road blocks\n\nKey Design Thinking Artifacts:\n\nHopes and Fears\nOpportunity Canvas\nStakeholder Map\nStakeholder Matrix\nAssumptions & Questions"
  }
]