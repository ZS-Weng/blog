[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaishan Weng",
    "section": "",
    "text": "About Myself\n\nI am techno-functional consultant and solutions architect with over 14 years of experience leading strategic digital transformation, data analytics and data science initiatives in the retail, government, health & security and restaurant industries. A dynamic & passionate professional who specializes in driving innovation, delivering large and complex projects, and advising senior executives on leveraging technology to drive business outcomes. A data scientist/engineer with proven track record in deploying impactful and productive AI products.\nI am passionate about the impact and difference technology has made and strive to explore ways of making existing work simpler and easier so that we can have the time to focus on bigger visions."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Zaishan’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nSome ideas on enhancing User Experience on Qlik Sense\n\n\n\n\n\n\n\nDesign Thinking\n\n\nRequirements Gathering\n\n\nData Analytics\n\n\nStakeholder Engagement\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nQuick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects\n\n\n\n\n\n\n\nDesign Thinking\n\n\nRequirements Gathering\n\n\nData Analytics\n\n\nStakeholder Engagement\n\n\nBusiness Intelligence\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nData cleaning on SAP data extracts in .txt format with Regex and Python\n\n\n\n\n\n\n\nData Engineering\n\n\nSAP\n\n\nPython\n\n\nRegex\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "",
    "text": "During one of our recent projects involving the procure to pay process, our team encountered SAP raw data extracted from the system in .txt format which proved to be difficult to clean using traditional methods like readlines() or split() by delimiters due to some inherent data inconsistencies.\nRegex matching proved to be helpful for such scenarios to clean the data."
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#common-sap-tables-in-procure-to-pay-process",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#common-sap-tables-in-procure-to-pay-process",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Common SAP tables in Procure to Pay process",
    "text": "Common SAP tables in Procure to Pay process\nFirst, lets go through what are common data tables extracted from SAP as part of the Procure to Pay process.\nAccounting Related Tables\n\nBKPF: Accounting Document Header\nBSAK: Accounting: Secondary Index for Vendors\nBSEG: Accounting Document Segment\n\nPurchase Order related Tables\n\nEKKO: Purchasing Document Header\nEKPO: Purchasing Document Item\nEKBE: History per Purchasing Document\nEKKN: Account Assignment in Purchasing Document\n\nMaterial Tables\n\nMAKT: Material Descriptions\n\nThere are various websites which provide additional information about the SAP tables.\n\nhttps://www.se80.co.uk/training-education/sap-tables/\nhttps://www.tcodesearch.com/sap-tables/detail?id=BSEG (Might need to enter through changing the id in URL without requiring premium membership)\n\n\n\n\nExample of Data Dictionary for BSEG Table Source: https://www.se80.co.uk/"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#format-of-sap-data-extract-in-.txt-file",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#format-of-sap-data-extract-in-.txt-file",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Format of SAP data extract in .txt file",
    "text": "Format of SAP data extract in .txt file\nFor our project, the output SAP data extracts is in a .txt format and with the typical structure as shown below:\n\nThe column header details starts at line 4\nThe width of each column is consistent between the column headers and the data for each file extracted\nActual data content starts at line 5 till the end\n\n\n\n\nSample SAP TXT Data Extract Structure with Mock Data\n\n\nThe sample SAP data in txt format and Jupyter Notebook can be found on GitHub: https://github.com/ZS-Weng/Data_Engineering/tree/main/Data_Cleaning\nThe two major data discrepancies encountered are:\n\nNewline character inserted in some of the fields\nPipe (|) delimiters found within the actual data"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#full-code-and-output",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#full-code-and-output",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Full Code and Output",
    "text": "Full Code and Output\nThe full working code for the data cleaning is as shown below:\nimport pandas as pd\nimport re\n\n# Read File\nwith open(\"Sample SAP Format.txt\", encoding=\"utf-8\") as f:\n    content_raw = f.read()\n\n# Clean extra newline characters\nnew_line_pattern = re.compile(\"([^1|-])[\\n](.)|(.)[\\n]([^|-])\")\ncontent_cleaned_newline = new_line_pattern.sub(r\"\\1 \\2\", content_raw)\ncontent_split_line = content_cleaned_newline.split(\"\\n\")\n\n# Clean the rest of content\n\n# Extract Header and Row Pattern\nheader_string = content_split_line[3]\ncolumn_header = [token.strip() for token in header_string.split(\"|\")][1:-1]\nlist_column_width = [\n    \"(.{\" + str(len(column)) + \"})\" for column in header_string.split(\"|\")\n][1:-1]\ncolumn_string_pattern = \"[|]\" + \"[|]\".join(list_column_width) + \"[|]\"\n#Extract Data Body\ncolumn_pattern = re.compile(column_string_pattern)\ncleaned_content = [\n    [token.strip() for token in column_pattern.match(row).groups()]\n    for row in content_split_line[5:-2]\n]\ndf_clean = pd.DataFrame(cleaned_content, columns=column_header)\nFinal pandas data frame output:\n\n\n\nFinal pandas tabular output after data cleaning"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#detailed-walk-through-of-the-codes",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#detailed-walk-through-of-the-codes",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Detailed Walk-through of the codes",
    "text": "Detailed Walk-through of the codes\nCleaning of newline character prior to splitting lines\nIn order to clean the (newline) characters which are not valid, we first use the read() instead of readline() method as the readline() method will split the lines by the character automatically.\nwith open(\"Sample SAP Format.txt\", encoding=\"utf-8\") as f:        \n    content_raw = f.read()\nNext, we use the Regex pattern ([1|-])[\\n](.)|(.)[\\n]([^|-]) to find and the invalid characters. The pattern basically detects newline that do not start with the characters 1 , | or - and do not end with the characters | or -.\nFor the invalid newline characters found, we replace them with a space using the .sub() method.\nnew_line_pattern = re.compile(\"([^1|-])[\\n](.)|(.)[\\n([^|-])\")\ncontent_cleaned_newline = new_line_pattern.sub(r\"\\1 \\2\",content_raw)\ncontent_split_line = content_cleaned_newline.split(\"\\n\")\nOutput after cleaning and splitting lines:\n\n\n\nList of Data After the Line Split\n\n\nData Cleaning Steps for Individual Line Based on the above, we can see that the data we are interested in is the column header (4th line) and the rest of the data content (6th to 2nd last lines).\nWe will first split the column by the pipe “|” delimiter and getting the columns width to create the Regex matching string pattern.\nheader_string = content_split_line[3]\ncolumn_header = [token.strip() for token in header_string.split(\"|\")][1:-1]\nlist_column_width = [\"(.{\" + str(len(column)) + \"})\" for column in header_string.split(\"|\")][1:-1]\ncolumn_string_pattern = \"[|]\" + \"[|]\".join(list_column_width) + \"[|]\"\nThe column_string_pattern to match that is generated for the sample data will be as follows:\n[|](.{10})[|](.{4})[|](.{3})[|](.{10})[|](.{20})[|]\nThis column pattern matching pattern is dynamically generated and will be unique for each of the data extract file even for the same tables as the width is adjusted during data extract according to the content.\nWe will then use the matching pattern to extract the rest of the data (6th to 2nd last line) with re.match().groups() instead of using the str.split() method.\ncolumn_pattern = re.compile(column_string_pattern)\ncleaned_content = [[token.strip() for token in column_pattern.match(row).groups()] for row in content_split_line[5:-2]]\nThe output after content splitting:\n\n\n\nData in Nested List format after splitting of content of each individual line\n\n\nHere we can see that the extra delimiters e.g. ‘Item A|B|C’ does not affect the content splitting .\nFinally, we combine the cleaned header columns and data into a pandas data frame.\ndf_clean = pd.DataFrame(cleaned_content, columns=column_header)\nFinal pandas Data Frame output:\n\n\n\nFinal cleaned data output in pandas Data Frame"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#useful-learning-resource-for-python-and-regex",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#useful-learning-resource-for-python-and-regex",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Useful Learning Resource for Python and Regex",
    "text": "Useful Learning Resource for Python and Regex\nWhen I was starting out, I found the book Automate the Boring Stuff with Python by Al Sweigart one of the best resources to learning about Python and Regex with many practical examples.\nThere is a free access option to the book on his website: https://automatetheboringstuff.com/ and this is the link to the specific chapter on Regex which formed the foundation on some of the implementation: https://automatetheboringstuff.com/2e/chapter7/"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "",
    "text": "After the successful delivery of a data analytics product, a common challenge faced is to enhance user adoption. One of the definitions of user adoption from an Indeed article is as follows:\nUser adoption rates are important because they tell a company or business how many users like the new product or version and how many don’t like it or do not try it. Most often, a higher adoption rate means a customer finds both value and ease in using the new product or version. Conversely, if a customer finds it requires too much effort to use or doesn’t add value, they may abandon the product altogether or stay with an older version.\nhttps://www.indeed.com/career-advice/career-development/user-adoption\nOne of the necessary ingredients to a successful adoption is the user experience on the interface. In this article, I would like to cover on ways we have found useful to make dashboards on Qlik Sense Apps more intuitive and user friendly.\nThe three areas I would like to cover are as follows:\n\nMenu Page for holistic view of overall health of business\n\nNavigation Bars for navigating between apps and within each app\nOnline Help and Information"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "",
    "text": "From my recent involvement in data analytics project engagements as the Data Science and Analytics Lead at IBM, it was observed that proper requirements gathering in the project initiation phase can make a big difference to the eventual success of the project.\nOne of the common challenges faced during this phase revolves around translating high level management visions at 50,000ft (e.g. Embark on Digital Transformation, Employ AI) into definitive and objective project requirements for implementation. From my experience, the use of appropriate design thinking artifacts can help to guide the process more effectively.\nIn this article, I would like to share a high level overview on the the design thinking artifacts which worked well on our projects and an example of how they are typically woven together during the project initiation phase. Most of these artifacts shared are aligned to IBM Design Thinking Methodology and additional details are available on the IBM Design Thinking website. https://www.ibm.com/design/thinking/ (signing up for a free IBM account might be required to access the materials)"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#breaking-down-the-initial-requirements-gathering-process",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#breaking-down-the-initial-requirements-gathering-process",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "Breaking down the Initial Requirements Gathering Process",
    "text": "Breaking down the Initial Requirements Gathering Process\nTypically the initial requirements gathering process can be broken down three main steps starting with crystallizing the overall business case of the project to the generation of project requirements. We will go through more details in each of the main steps:\n\nStep 1- Defining goals and objectives of the project\nStep 2- Understanding users, their current work process and envisioned to-be state\nStep 3- Initial formulation of analytics solution requirements\n\n\nStep 1: Defining Goals and Objectives of the Project\nThe first step usually involves connecting with the project sponsor and key stakeholders who will be responsible for the outcome of the project. It is critical for the stakeholders and project team to come together and define the overall direction of the project.\nTypical Stakeholders from Client: Project Sponsor, Product Owner, Project Manager, SMEs, Users\nKey Objectives:\n\nEstablish overall business case for the project including potential benefits and how success of the project will be measured\nIdentify and understand key stakeholders on the project\nIdentify risks and potential road blocks\n\nKey Design Thinking Artifacts:\n\nHopes and Fears\nOpportunity Canvas\nStakeholder Map\nStakeholder Matrix\nAssumptions & Questions\n\n\nHopes and Fears\nThe Hopes and Fears artifact is useful as a warm up exercise to gather inputs from the stakeholders about their hopes for the project and gain insights to their worries and concerns. At times, the fears section might also help to uncover potential pitfalls and lessons learnt from previous projects.\n\n\n\nSample of a completed Hopes & Fears artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/hopes-and-fears\n\n\n\n\nOpportunity Canvas\nOpportunity Canvas is a very helpful artifact to provide holistic view of a project. The canvas can provide assessment on the overall readiness and identify areas with potential gaps and challenges. It is a working document that will be constantly refined throughout the requirements gathering phase. \nIn our engagements, we have further expanded and customized the sections of the opportunity canvas:\n\nProblems/Goals to be Solved\nSolution Ideas\nValue Proposition\nKey Metrics\nTarget Customer/User Type Segments\nHigh Level Business Benefits and Impact\nCompetitive Landscape\nDistribution Channel/Adoption Strategy\nKey Partners/Alliances\nCost Structure\nAssumptions/Questions\nConstraints\nImpediments\nSquad Members Required\nBU Support Required\nAssets & Accelerators\n\n\n\nStakeholder Map & Stakeholder Matrix\nA better understanding of stakeholders involved is critical to garner support for the project and ensure all relevant stakeholders are sufficiently engaged. The first part of the exercise will involve listing down all potential stakeholders, grouping them and identifying the relationships.\n\n\n\nSample Stakeholder Mapping from https://www.ibm.com/design/thinking/page/toolkit/activity/stakeholder-map\n\n\nIn addition to the stakeholder map, the stakeholder matrix provides another useful perspective by locating each stakeholder in the 2 by 2 matrix of Influence vs Interest.\nThe stakeholder map and matrix can also help to customize the communications and engagement plan for the various stakeholder groups. (e.g. type of updates, frequency of updates, attendance at meetings etc.)\n\n\n\n2 X 2 Stakeholder Matrix of Influence vs Interest\n\n\n\n\nAssumptions and Questions\nThe Assumptions and Questions chart can be prefilled with relevant information throughout the earlier discussions. At this stage, the chart can help to recap on existing assumptions, add new details that were missed and perform an initial assessment on the impact and probability as a group.\n\n\n\nSample Assumptions and Questions artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/assumptions-and-questions\n\n\n\n\n\nStep 2: Understanding users, their current work process and pain points\nWith an alignment on the overall business case and strategic direction of the project, we will shift gears in Step 2 to understand more about the end users and details around their work processes in the current and future context.\nTypical Stakeholders from Client: SMEs and Users from the different groups who will be the end users\nKey Objectives:\n\nUnderstanding the end users, their current work process and how they are expected to interact with the solution in their work process\nIdentify wish lists and pain points\n\nKey Design Thinking Artifacts:\n\nEmpathy Map\nTo-Be Scenario Map\n\n\nEmpathy Map\nThe empathy map aids in the understanding of the profile for the users. It is best practice to generate empathy maps for each group of users. The collection of details can be either in the context of the problem statement or in the wider context of their day to day work. The details of what they say, do, think and feel will help to paint a representative user profile of the group.\n\n\n\nSample Empathy Map artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/empathy-map\n\n\n\n\nTo-Be Scenario Map\nBuilding upon the empathy map for each user group, additional details on each phase of the to-be working process can be consolidated to form the To-Be scenario map. The discussion with the users in the dimensions of doing, thinking and feeling for each of the phases will help to uncover pain points, gaps and potential benefits.\n\n\n\nSample To-Be Scenario Map artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/to-be-scenario-map\n\n\n\n\n\nStep 3- Initial formulation of analytics solution requirements\nIn Step 3, we will build upon the established direction and understanding of users and their needs & wants. The key goal in this phase is to develop the solution requirements in sufficient details to start the project.\nTypical Stakeholders from Client: SMEs and Users from the different groups who will be the end users\nKey Objectives:\n\nDeveloping high level requirements in hills or user stories format\nBrainstorm on functional features\nPrioritize features for development\n\nKey Design Thinking Artifacts: * Hills (User Stories) * Big Idea * Prioritization Map\n\nUser Stories or Hills Writing (Who, What, Wow)\nThe gathering of User Stories or Hills captures user requirements the following format:\n\nWho (In the capacity of a certain Role),\nWhat (I would like to take some action),\nWow (To achieve a certain outcome).\n\nThis will be useful to identify target users, identify user interactions with the solution and understand the underlying purpose and outcome to be achieved.\n\n\n\nSample Hills Writing Exercise output from https://www.ibm.com/design/thinking/page/toolkit/activity/writing-hills\n\n\n\n\nBig Idea\nBased on the user stories & hills captured, we can conduct a brainstorming session to draw out ideas on features required in each of the user stories & hills. Similar features can be categorized and grouped together to form a coherent functionality set.\n\n\n\nSample Big Idea Collections from https://www.ibm.com/design/thinking/page/toolkit/activity/big-idea-vignettes\n\n\n\n\nPrioritization Map\nThe next step involves placing the features and functionality on the prioritization map. Features in the No Brainers section with the highest impact and feasibility will be planned with the highest priority for development first. This will help the project to deliver benefits early on. There is usually some deliberation involve in the prioritization between the big bets section (High value but might be less feasible) and utilities section (Higher feasibility but might deliver less value). The features in the un-wise section with low impact and feasibility will have least lower priority and are usually parked for future review.\n\n\n\nSample Prioritization Grid from https://www.ibm.com/design/thinking/page/toolkit/activity/prioritization"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#transiting-from-project-initiation-phase-to-project-planning-phase",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#transiting-from-project-initiation-phase-to-project-planning-phase",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "Transiting from Project Initiation Phase to Project Planning Phase",
    "text": "Transiting from Project Initiation Phase to Project Planning Phase\nA logical transition point from the project initiation phase to the project planning phase will be the step where the details from the prioritization map is converted into the product backlog.\n\nProduct Backlog Hierarchy\nA typical product backlog can consist of various layers depending on the complexity of the project and terminology that the team or existing tool follows. Using an example from Azure DevOps board, a typical structure on the product backlog will be: * Epic * Feature * User Story (Product Backlog Item) * Task\n\n\n\nAzure Dev Ops Board Standard Scrum Backlog Structure\n\n\nUsing the inputs from the prioritization map, the team will work closely with the product owner and users to develop the product backlog. Based on experience, most of the information captured will be at the level of features and product backlog items. The features can be grouped into Epics signifying features for each product release and broken down into product backlog items.\nThe rest of the project planning activities will follow on from this phase.\n\n\nExample of Product Backlog Items for Data Visualization Dashboard\nFor data visualization features, there are three standard components to be captured as part of the product backlog as shown below:\n\n\n\nStandard Components of a Data Visualization Workflow\n\n\nFor most dashboard visualization requirements on BI (Business Intelligence) tools such as Tableau, Qlik, PowerBI, the standard functional requirements that needs to be defined are as follows:\nAnalytics Flow: The usual analytics process usually presents information in a top-down approach, moving from a big picture summary to detailed report with many rows of data. An example of this is Qlik’s DAR(Dashboard, Analysis, Reporting) methodology where information in presented from the highest granularity to the lowest granularity.\nDimensions: Identifying the dimensions /features is a critical component of the function visualization requirements. From the identified dimensions, the lowest granularity of the data and a hierarchy can be derived. In addition, selected dimensions can be defined as filters. A subset of commonly used dimensions and their hierarchies in a retail context is shown below:\n\n\n\nSample of common dimensions in the retail context\n\n\nKPIs: KPIs are any of the columns that can provide a quantitative measure against the dimensions. It is important to capture the business rules and transformations clearly and ensure that it is aligned across the different group of stakeholders. A subset of commonly used metrics in a retail context is as shown below:\n\n\n\nSample of commonly used KPIs in the retail context\n\n\nThanks for reading and hope the information was useful in some way!"
  }
]