[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaishan Weng",
    "section": "",
    "text": "I am techno-functional consultant and solutions architect with over 14 years of experience leading strategic digital transformation, data analytics and data science initiatives in the retail, government, health & security and restaurant industries. A dynamic & passionate professional who specializes in driving innovation, delivering large and complex projects, and advising senior executives on leveraging technology to drive business outcomes. A data scientist/engineer with proven track record in deploying impactful and productive AI products.\nI am passionate about the impact and difference technology has made and strive to explore ways of making existing work simpler and easier so that we can have the time to focus on bigger visions.\n\n\nCheck out my blog post on some of the Data Engineering, Data Science and Data Visualization learnings from implementing them in various projects here"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Zaishan’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nHierarchical Clustering can be more suitable compared to KMeans when grouping customers based on purchase behaviors and detecting outliers\n\n\n\n\n\n\n\nClustering\n\n\nMachine Learning\n\n\nAnomaly Detection\n\n\nData Science\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2023\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nApplications of Regex and Python in data transformation for masking of sensitive information and extraction of date details from free text\n\n\n\n\n\n\n\nRegex\n\n\nData Engineering\n\n\nData Masking\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nImpressive yet easy to implement Document Understanding system with OCR-free Donut Transformers Model in Python\n\n\n\n\n\n\n\nDocVQA\n\n\nData Science\n\n\nAI\n\n\nMachine Learning\n\n\nTransformer\n\n\nDocument Understanding\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nHandle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas\n\n\n\n\n\n\n\nPython\n\n\nData Cleaning\n\n\nExcel\n\n\nCSV\n\n\nData Engineering\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nSome ideas on enhancing User Experience on Qlik Sense\n\n\n\n\n\n\n\nQlik\n\n\nUser Experience\n\n\nBusiness Intelligence\n\n\nUser Interface\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nQuick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects\n\n\n\n\n\n\n\nDesign Thinking\n\n\nRequirements Gathering\n\n\nData Analytics\n\n\nStakeholder Engagement\n\n\nBusiness Intelligence\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nData cleaning on SAP data extracts in .txt format with Regex and Python\n\n\n\n\n\n\n\nData Engineering\n\n\nSAP\n\n\nPython\n\n\nRegex\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "",
    "text": "During one of our recent projects involving the procure to pay process, our team encountered SAP raw data extracted from the system in .txt format which proved to be difficult to clean using traditional methods like readlines() or split() by delimiters due to some inherent data inconsistencies.\nRegex matching proved to be helpful for such scenarios to clean the data."
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#common-sap-tables-in-procure-to-pay-process",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#common-sap-tables-in-procure-to-pay-process",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Common SAP tables in Procure to Pay process",
    "text": "Common SAP tables in Procure to Pay process\nFirst, lets go through what are common data tables extracted from SAP as part of the Procure to Pay process.\nAccounting Related Tables\n\nBKPF: Accounting Document Header\nBSAK: Accounting: Secondary Index for Vendors\nBSEG: Accounting Document Segment\n\nPurchase Order related Tables\n\nEKKO: Purchasing Document Header\nEKPO: Purchasing Document Item\nEKBE: History per Purchasing Document\nEKKN: Account Assignment in Purchasing Document\n\nMaterial Tables\n\nMAKT: Material Descriptions\n\nThere are various websites which provide additional information about the SAP tables.\n\nhttps://www.se80.co.uk/training-education/sap-tables/\nhttps://www.tcodesearch.com/sap-tables/detail?id=BSEG (Might need to enter through changing the id in URL without requiring premium membership)\n\n\n\n\nExample of Data Dictionary for BSEG Table Source: https://www.se80.co.uk/"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#format-of-sap-data-extract-in-.txt-file",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#format-of-sap-data-extract-in-.txt-file",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Format of SAP data extract in .txt file",
    "text": "Format of SAP data extract in .txt file\nFor our project, the output SAP data extracts is in a .txt format and with the typical structure as shown below:\n\nThe column header details starts at line 4\nThe width of each column is consistent between the column headers and the data for each file extracted\nActual data content starts at line 5 till the end\n\n\n\n\nSample SAP TXT Data Extract Structure with Mock Data\n\n\nThe sample SAP data in txt format and Jupyter Notebook can be found on GitHub: https://github.com/ZS-Weng/Data_Engineering/tree/main/Data_Cleaning\nThe two major data discrepancies encountered are:\n\nNewline character inserted in some of the fields\nPipe (|) delimiters found within the actual data"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#full-code-and-output",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#full-code-and-output",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Full Code and Output",
    "text": "Full Code and Output\nThe full working code for the data cleaning is as shown below:\nimport pandas as pd\nimport re\n\n# Read File\nwith open(\"Sample SAP Format.txt\", encoding=\"utf-8\") as f:\n    content_raw = f.read()\n\n# Clean extra newline characters\nnew_line_pattern = re.compile(\"([^1|-])[\\n](.)|(.)[\\n]([^|-])\")\ncontent_cleaned_newline = new_line_pattern.sub(r\"\\1 \\2\", content_raw)\ncontent_split_line = content_cleaned_newline.split(\"\\n\")\n\n# Clean the rest of content\n\n# Extract Header and Row Pattern\nheader_string = content_split_line[3]\ncolumn_header = [token.strip() for token in header_string.split(\"|\")][1:-1]\nlist_column_width = [\n    \"(.{\" + str(len(column)) + \"})\" for column in header_string.split(\"|\")\n][1:-1]\ncolumn_string_pattern = \"[|]\" + \"[|]\".join(list_column_width) + \"[|]\"\n#Extract Data Body\ncolumn_pattern = re.compile(column_string_pattern)\ncleaned_content = [\n    [token.strip() for token in column_pattern.match(row).groups()]\n    for row in content_split_line[5:-2]\n]\ndf_clean = pd.DataFrame(cleaned_content, columns=column_header)\nFinal pandas data frame output:\n\n\n\nFinal pandas tabular output after data cleaning"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#detailed-walk-through-of-the-codes",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#detailed-walk-through-of-the-codes",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Detailed Walk-through of the codes",
    "text": "Detailed Walk-through of the codes\nCleaning of newline character prior to splitting lines\nIn order to clean the (newline) characters which are not valid, we first use the read() instead of readline() method as the readline() method will split the lines by the character automatically.\nwith open(\"Sample SAP Format.txt\", encoding=\"utf-8\") as f:        \n    content_raw = f.read()\nNext, we use the Regex pattern ([1|-])[\\n](.)|(.)[\\n]([^|-]) to find and the invalid characters. The pattern basically detects newline that do not start with the characters 1 , | or - and do not end with the characters | or -.\nFor the invalid newline characters found, we replace them with a space using the .sub() method.\nnew_line_pattern = re.compile(\"([^1|-])[\\n](.)|(.)[\\n([^|-])\")\ncontent_cleaned_newline = new_line_pattern.sub(r\"\\1 \\2\",content_raw)\ncontent_split_line = content_cleaned_newline.split(\"\\n\")\nOutput after cleaning and splitting lines:\n\n\n\nList of Data After the Line Split\n\n\nData Cleaning Steps for Individual Line Based on the above, we can see that the data we are interested in is the column header (4th line) and the rest of the data content (6th to 2nd last lines).\nWe will first split the column by the pipe “|” delimiter and getting the columns width to create the Regex matching string pattern.\nheader_string = content_split_line[3]\ncolumn_header = [token.strip() for token in header_string.split(\"|\")][1:-1]\nlist_column_width = [\"(.{\" + str(len(column)) + \"})\" for column in header_string.split(\"|\")][1:-1]\ncolumn_string_pattern = \"[|]\" + \"[|]\".join(list_column_width) + \"[|]\"\nThe column_string_pattern to match that is generated for the sample data will be as follows:\n[|](.{10})[|](.{4})[|](.{3})[|](.{10})[|](.{20})[|]\nThis column pattern matching pattern is dynamically generated and will be unique for each of the data extract file even for the same tables as the width is adjusted during data extract according to the content.\nWe will then use the matching pattern to extract the rest of the data (6th to 2nd last line) with re.match().groups() instead of using the str.split() method.\ncolumn_pattern = re.compile(column_string_pattern)\ncleaned_content = [[token.strip() for token in column_pattern.match(row).groups()] for row in content_split_line[5:-2]]\nThe output after content splitting:\n\n\n\nData in Nested List format after splitting of content of each individual line\n\n\nHere we can see that the extra delimiters e.g. ‘Item A|B|C’ does not affect the content splitting .\nFinally, we combine the cleaned header columns and data into a pandas data frame.\ndf_clean = pd.DataFrame(cleaned_content, columns=column_header)\nFinal pandas Data Frame output:\n\n\n\nFinal cleaned data output in pandas Data Frame"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#useful-learning-resource-for-python-and-regex",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#useful-learning-resource-for-python-and-regex",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Useful Learning Resource for Python and Regex",
    "text": "Useful Learning Resource for Python and Regex\nWhen I was starting out, I found the book Automate the Boring Stuff with Python by Al Sweigart one of the best resources to learning about Python and Regex with many practical examples.\nThere is a free access option to the book on his website: https://automatetheboringstuff.com/ and this is the link to the specific chapter on Regex which formed the foundation on some of the implementation: https://automatetheboringstuff.com/2e/chapter7/\nThanks for reading and hope the information was useful in some way!"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "",
    "text": "After the successful delivery of a data analytics product, a common challenge faced is to enhance user adoption. One of the definitions of user adoption from an Indeed article is as follows:\nUser adoption rates are important because they tell a company or business how many users like the new product or version and how many don’t like it or do not try it. Most often, a higher adoption rate means a customer finds both value and ease in using the new product or version. Conversely, if a customer finds it requires too much effort to use or doesn’t add value, they may abandon the product altogether or stay with an older version.\nhttps://www.indeed.com/career-advice/career-development/user-adoption\nOne of the necessary ingredients to a successful adoption is the user experience on the interface. In this article, I would like to cover on ways we have found useful to make dashboards on Qlik Sense Apps more intuitive and user friendly.\nThe three areas I would like to cover are as follows:\n\nMenu Page for holistic view of overall health of business\n\n\n\n\nExample of Menu Page with KPIs on Qlik Sense\n\n\n\nNavigation Bars for navigating between apps and within each app\n\n\n\n\nNavigation between App and Within App\n\n\n\nOnline Help and Information\n\n\n\n\nHelp Page Built In Online with the App\n\n\nThe Qlik Sense demo app can be found in the following Github link:\nhttps://github.com/ZS-Weng/QlikSense/blob/main/QlikSense%20Navigation.qvf"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "",
    "text": "From my recent involvement in data analytics project engagements as the Data Science and Analytics Lead at IBM, it was observed that proper requirements gathering in the project initiation phase can make a big difference to the eventual success of the project.\nOne of the common challenges faced during this phase revolves around translating high level management visions at 50,000ft (e.g. Embark on Digital Transformation, Employ AI) into definitive and objective project requirements for implementation. From my experience, the use of appropriate design thinking artifacts can help to guide the process more effectively.\nIn this article, I would like to share a high level overview on the the design thinking artifacts which worked well on our projects and an example of how they are typically woven together during the project initiation phase. Most of these artifacts shared are aligned to IBM Design Thinking Methodology and additional details are available on the IBM Design Thinking website. https://www.ibm.com/design/thinking/ (signing up for a free IBM account might be required to access the materials)"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#breaking-down-the-initial-requirements-gathering-process",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#breaking-down-the-initial-requirements-gathering-process",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "Breaking down the Initial Requirements Gathering Process",
    "text": "Breaking down the Initial Requirements Gathering Process\nTypically the initial requirements gathering process can be broken down three main steps starting with crystallizing the overall business case of the project to the generation of project requirements. We will go through more details in each of the main steps:\n\nStep 1- Defining goals and objectives of the project\nStep 2- Understanding users, their current work process and envisioned to-be state\nStep 3- Initial formulation of analytics solution requirements\n\n\nStep 1: Defining Goals and Objectives of the Project\nThe first step usually involves connecting with the project sponsor and key stakeholders who will be responsible for the outcome of the project. It is critical for the stakeholders and project team to come together and define the overall direction of the project.\nTypical Stakeholders from Client: Project Sponsor, Product Owner, Project Manager, SMEs, Users\nKey Objectives:\n\nEstablish overall business case for the project including potential benefits and how success of the project will be measured\nIdentify and understand key stakeholders on the project\nIdentify risks and potential road blocks\n\nKey Design Thinking Artifacts:\n\nHopes and Fears\nOpportunity Canvas\nStakeholder Map\nStakeholder Matrix\nAssumptions & Questions\n\n\nHopes and Fears\nThe Hopes and Fears artifact is useful as a warm up exercise to gather inputs from the stakeholders about their hopes for the project and gain insights to their worries and concerns. At times, the fears section might also help to uncover potential pitfalls and lessons learnt from previous projects.\n\n\n\nSample of a completed Hopes & Fears artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/hopes-and-fears\n\n\n\n\nOpportunity Canvas\nOpportunity Canvas is a very helpful artifact to provide holistic view of a project. The canvas can provide assessment on the overall readiness and identify areas with potential gaps and challenges. It is a working document that will be constantly refined throughout the requirements gathering phase. \nIn our engagements, we have further expanded and customized the sections of the opportunity canvas:\n\nProblems/Goals to be Solved\nSolution Ideas\nValue Proposition\nKey Metrics\nTarget Customer/User Type Segments\nHigh Level Business Benefits and Impact\nCompetitive Landscape\nDistribution Channel/Adoption Strategy\nKey Partners/Alliances\nCost Structure\nAssumptions/Questions\nConstraints\nImpediments\nSquad Members Required\nBU Support Required\nAssets & Accelerators\n\n\n\nStakeholder Map & Stakeholder Matrix\nA better understanding of stakeholders involved is critical to garner support for the project and ensure all relevant stakeholders are sufficiently engaged. The first part of the exercise will involve listing down all potential stakeholders, grouping them and identifying the relationships.\n\n\n\nSample Stakeholder Mapping from https://www.ibm.com/design/thinking/page/toolkit/activity/stakeholder-map\n\n\nIn addition to the stakeholder map, the stakeholder matrix provides another useful perspective by locating each stakeholder in the 2 by 2 matrix of Influence vs Interest.\nThe stakeholder map and matrix can also help to customize the communications and engagement plan for the various stakeholder groups. (e.g. type of updates, frequency of updates, attendance at meetings etc.)\n\n\n\n2 X 2 Stakeholder Matrix of Influence vs Interest\n\n\n\n\nAssumptions and Questions\nThe Assumptions and Questions chart can be prefilled with relevant information throughout the earlier discussions. At this stage, the chart can help to recap on existing assumptions, add new details that were missed and perform an initial assessment on the impact and probability as a group.\n\n\n\nSample Assumptions and Questions artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/assumptions-and-questions\n\n\n\n\n\nStep 2: Understanding users, their current work process and pain points\nWith an alignment on the overall business case and strategic direction of the project, we will shift gears in Step 2 to understand more about the end users and details around their work processes in the current and future context.\nTypical Stakeholders from Client: SMEs and Users from the different groups who will be the end users\nKey Objectives:\n\nUnderstanding the end users, their current work process and how they are expected to interact with the solution in their work process\nIdentify wish lists and pain points\n\nKey Design Thinking Artifacts:\n\nEmpathy Map\nTo-Be Scenario Map\n\n\nEmpathy Map\nThe empathy map aids in the understanding of the profile for the users. It is best practice to generate empathy maps for each group of users. The collection of details can be either in the context of the problem statement or in the wider context of their day to day work. The details of what they say, do, think and feel will help to paint a representative user profile of the group.\n\n\n\nSample Empathy Map artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/empathy-map\n\n\n\n\nTo-Be Scenario Map\nBuilding upon the empathy map for each user group, additional details on each phase of the to-be working process can be consolidated to form the To-Be scenario map. The discussion with the users in the dimensions of doing, thinking and feeling for each of the phases will help to uncover pain points, gaps and potential benefits.\n\n\n\nSample To-Be Scenario Map artifact from https://www.ibm.com/design/thinking/page/toolkit/activity/to-be-scenario-map\n\n\n\n\n\nStep 3- Initial formulation of analytics solution requirements\nIn Step 3, we will build upon the established direction and understanding of users and their needs & wants. The key goal in this phase is to develop the solution requirements in sufficient details to start the project.\nTypical Stakeholders from Client: SMEs and Users from the different groups who will be the end users\nKey Objectives:\n\nDeveloping high level requirements in hills or user stories format\nBrainstorm on functional features\nPrioritize features for development\n\nKey Design Thinking Artifacts: * Hills (User Stories) * Big Idea * Prioritization Map\n\nUser Stories or Hills Writing (Who, What, Wow)\nThe gathering of User Stories or Hills captures user requirements the following format:\n\nWho (In the capacity of a certain Role),\nWhat (I would like to take some action),\nWow (To achieve a certain outcome).\n\nThis will be useful to identify target users, identify user interactions with the solution and understand the underlying purpose and outcome to be achieved.\n\n\n\nSample Hills Writing Exercise output from https://www.ibm.com/design/thinking/page/toolkit/activity/writing-hills\n\n\n\n\nBig Idea\nBased on the user stories & hills captured, we can conduct a brainstorming session to draw out ideas on features required in each of the user stories & hills. Similar features can be categorized and grouped together to form a coherent functionality set.\n\n\n\nSample Big Idea Collections from https://www.ibm.com/design/thinking/page/toolkit/activity/big-idea-vignettes\n\n\n\n\nPrioritization Map\nThe next step involves placing the features and functionality on the prioritization map. Features in the No Brainers section with the highest impact and feasibility will be planned with the highest priority for development first. This will help the project to deliver benefits early on. There is usually some deliberation involve in the prioritization between the big bets section (High value but might be less feasible) and utilities section (Higher feasibility but might deliver less value). The features in the un-wise section with low impact and feasibility will have least lower priority and are usually parked for future review.\n\n\n\nSample Prioritization Grid from https://www.ibm.com/design/thinking/page/toolkit/activity/prioritization"
  },
  {
    "objectID": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#transiting-from-project-initiation-phase-to-project-planning-phase",
    "href": "posts/2022-07-08-design-thinking-quick-start-guide/index.html#transiting-from-project-initiation-phase-to-project-planning-phase",
    "title": "Quick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects",
    "section": "Transiting from Project Initiation Phase to Project Planning Phase",
    "text": "Transiting from Project Initiation Phase to Project Planning Phase\nA logical transition point from the project initiation phase to the project planning phase will be the step where the details from the prioritization map is converted into the product backlog.\n\nProduct Backlog Hierarchy\nA typical product backlog can consist of various layers depending on the complexity of the project and terminology that the team or existing tool follows. Using an example from Azure DevOps board, a typical structure on the product backlog will be: * Epic * Feature * User Story (Product Backlog Item) * Task\n\n\n\nAzure Dev Ops Board Standard Scrum Backlog Structure\n\n\nUsing the inputs from the prioritization map, the team will work closely with the product owner and users to develop the product backlog. Based on experience, most of the information captured will be at the level of features and product backlog items. The features can be grouped into Epics signifying features for each product release and broken down into product backlog items.\nThe rest of the project planning activities will follow on from this phase.\n\n\nExample of Product Backlog Items for Data Visualization Dashboard\nFor data visualization features, there are three standard components to be captured as part of the product backlog as shown below:\n\n\n\nStandard Components of a Data Visualization Workflow\n\n\nFor most dashboard visualization requirements on BI (Business Intelligence) tools such as Tableau, Qlik, PowerBI, the standard functional requirements that needs to be defined are as follows:\nAnalytics Flow: The usual analytics process usually presents information in a top-down approach, moving from a big picture summary to detailed report with many rows of data. An example of this is Qlik’s DAR(Dashboard, Analysis, Reporting) methodology where information in presented from the highest granularity to the lowest granularity.\nDimensions: Identifying the dimensions /features is a critical component of the function visualization requirements. From the identified dimensions, the lowest granularity of the data and a hierarchy can be derived. In addition, selected dimensions can be defined as filters. A subset of commonly used dimensions and their hierarchies in a retail context is shown below:\n\n\n\nSample of common dimensions in the retail context\n\n\nKPIs: KPIs are any of the columns that can provide a quantitative measure against the dimensions. It is important to capture the business rules and transformations clearly and ensure that it is aligned across the different group of stakeholders. A subset of commonly used metrics in a retail context is as shown below:\n\n\n\nSample of commonly used KPIs in the retail context\n\n\nThanks for reading and hope the information was useful in some way!"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#menu-page-for-holistic-view-of-overall-health-of-business",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#menu-page-for-holistic-view-of-overall-health-of-business",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "Menu Page for holistic view of overall health of business",
    "text": "Menu Page for holistic view of overall health of business\nIn a data analytics / business intelligence product, it usually can consist of several apps or reports based on different aspects of the business. Inspired by Fitbit’s health analysis charts, the menu page for a Qlik Sense dashboard is customized with icons and the most important KPI. Based on the thresholds set, users can have an holistic understanding on the overall health summary from various aspects of the business.\nThe picture below illustrates how the color of the KPI and icon will change based on the changing value. The threshold set is 33 and 66.\n\n\n\nMenu Page showing how the color of the KPI and Icons will change based on the KPI value and threshold\n\n\nQlik Sense Codes: The thresholding can easily be setup in Qlik Sense with if statements.\n\nCode for controlling the colors on the KPI and Icons in Qlik Sense:\n\n=if(v_KPI3 <= 33, ‘#e8291c’,\nif( v_KPI3 > 33 and v_KPI3 <= 66 , ‘#abaa20’, \nif( v_KPI3 > 66, ‘#25af2f’)))\n\nCode for controlling the Icon Image in Qlik Sense:\n\n=if(v_KPI3 <= 33, ‘Warning triangle’,\nif( v_KPI3 > 33 and v_KPI3 <= 66 , ‘View’, \nif( v_KPI3 > 66, ‘Tick’)))\nOther resources for creating the icons\n\nFree icons can be downloaded from: https://icons8.com/\nThe websites that provides reference on beautiful gradient color combinations: https://uigradients.com/ , https://digitalsynopsis.com/design/beautiful-color-ui-gradients-backgrounds/\nOnline tools that can invert a black and white icon: https://www.google.com/search?q=image+invert\nPython Code that can invert a black and white icon:\n\nimport os\nfrom PIL import Image, ImageOps\nfrom matplotlib.pyplot import imshow\nfrom pathlib import Path\n\n\"\"\"\nSetup Base and Process Folder\n\"\"\"\n# Enter base folder path\nbase_folder = Path(\"./icons\")\nimage_paths = Path.glob(base_folder, \"*.png\")\nprocessed_folder = \"./icons/processed/\"\nos.makedirs(processed_folder, exist_ok=True)\n\n\ndef image_invert(image_path):\n    \"\"\"\n    Perform image inversion of the colots and set transparent map\n    for black pixels\n    \"\"\"\n    image = Image.open(image_path)\n    if image.mode == \"RGBA\":\n        r, g, b, a = image.split()\n        rgb_image = Image.merge(\"RGB\", (r, g, b))\n        inverted_image = ImageOps.invert(rgb_image)\n        r2, g2, b2 = inverted_image.split()\n        final_transparent_image = Image.merge(\"RGBA\", (r2, g2, b2, a))\n        return final_transparent_image\n\n    else:\n        inverted_image = ImageOps.invert(image)\n        return inverted_image\n\n\nfor image_path in image_paths:\n    inverted_image = image_invert(image_path)\n    newpath = Path.joinpath(base_folder, \"processed\", image_path.stem + \"_inverted.png\")\n    inverted_image.save(newpath)"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#navigation-bars-between-apps-and-within-each-app",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#navigation-bars-between-apps-and-within-each-app",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "Navigation Bars between apps and within each app",
    "text": "Navigation Bars between apps and within each app\nTo make the navigation across apps and within an app more intuitive, buttons can be used to create the respective navigations. In our projects, we aim to create an experience similar to navigating websites with the side bars providing navigation between apps and top bars within each app. The top bars for our case are standardized to enable users to drill down from Summary > Analysis > Details.\n\n\n\nExample of Navigation between the Apps\n\n\nSample Code in Qlik Sense on Variable Setup:\nSet v_TopBarSelectedColor = ‘#8351a8’;\nSet v_TopBarUnSelectedColor = ‘#4d7b93’;\nSet v_SideBarSelectedColor = ‘#8351a8’;\nSet v_SideBarUnSelectedColor = ‘#4d7b93’;\nSet v_SideBar1Text = ‘App 1’;\nSet v_SideBar2Text = ‘App 2’;\nSet v_SideBar3Text = ‘App 3’;\nSet v_SideBar4Text = ‘Help Page’;\nSet v_SideBar1Link = ‘a0d70b49–7a21–44d0-b9a9-daaa0701c175’;\nSet v_SideBar2Link = ‘e03b4a67–89c1–425b-852c-44d112e48d7c’;\nSet v_SideBar3Link = ‘adc68de9–5a42–40ef-bdcf-dffa2981d83c’;\nSet v_SideBar4Link = ‘f3e454e2-ea27–46ef-bc5a-d1ddd280aaf9’;\nSet v_helppageLink = ‘f3e454e2-ea27–46ef-bc5a-d1ddd280aaf9’;\nSet v_menupagelink = ‘be0a8400–5df7–4783–8339–1e44e66b04e5’;"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#online-help-and-information",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#online-help-and-information",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "Online Help and Information",
    "text": "Online Help and Information\nIn many projects, often training materials are shared in PowerPoint decks or word documents. These documents might not be easily accessible to every user and it may become challenging at times for users to find the right reference guide when they need help while navigating an analytics app. In our projects, we recommend for the help pages to be built in within as an accompanying sheet or app so that it will be online together with the analytics product.\n\n\n\nNavigation to Help Page and use of Containers as Step by Step Guide"
  },
  {
    "objectID": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#conclusion",
    "href": "posts/2022-07-22-Enhance-UX-on-Qlik/index.html#conclusion",
    "title": "Some ideas on enhancing User Experience on Qlik Sense",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, I have aimed to share how user experience can be enhanced on Qlik Sense applications with out of the box capabilities.\nAny further enhancements will likely require tools beyond the application builder like Mashups or D3.js custom visualizations.\nThanks for reading and hope the information was useful in some way!"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "",
    "text": "Image generated from Hugging Face Stable Diffusion with the prompt “multitude of sources coming together in a mural”"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#introduction",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#introduction",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "Introduction",
    "text": "Introduction\nIn the process of ingesting and cleaning raw data in various formats using python, there were some tricks found to be useful based on the issues which were encountered. Types of source files covered in the article so far are as follows:\n\nxlsb (excel binary format)\ncsv files where encoding are not UTF-8\nexcel files with multiple tabs\nraw data files in .txt format\nparquet files\n\nThe corresponding notebook and sample data can be found at https://github.com/ZS-Weng/Data_Engineering/tree/main/Data_Cleaning"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#reading-in-excel-files-in-.xlsb-format",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#reading-in-excel-files-in-.xlsb-format",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "1. Reading in excel files in .xlsb format",
    "text": "1. Reading in excel files in .xlsb format\nThere are various formats of excel files e.g. .xls, .xlsx, .xlsb, .xlsm etc. Based on the out of the box functionality from Pandas, the formats .xlsx and .xls are supported and the .xlsb (binary) format is not supported out of the box.\nThe most efficient solution I found is to install the pyxlsb extension. More details can be found at the PyPI project website: https://pypi.org/project/pyxlsb/\nThe code to execute including installation of the library will be as follows:\npip install pyxlsb\ndf = pd.read_excel(file, engine=”pyxlsb”)"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#resolve-unicodedecodeerror-during-csv-file-read",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#resolve-unicodedecodeerror-during-csv-file-read",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "2. Resolve UnicodeDecodeError during CSV file read",
    "text": "2. Resolve UnicodeDecodeError during CSV file read\nMost of the files are encoded in “UTF-8” which is the default encoding. However, there are times when when the encoding in not “UTF-8” and there will be an error message similar to this:\nUnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xff in position 0: invalid start byte\nThe most effective method I have encountered is to first read the file as binary using ‘rb’ mode to determine the encoding, then apply the encoding which was detected.\nimport chardet\nwith open(file, 'rb') as f:\n    encoding = chardet.detect(f.read())\nprint(encoding)\nThe result will be something like this:\n\n\n\nDetecting encoding in a file\n\n\nBased on the encoding identified, we can use the encoding in pd.read_csv()\ndf = pd.read_csv(file, encoding=encoding['encoding'])\nThe result will look something like this:\n\n\n\nReading the csv file based on updated encoding"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#extracting-data-from-multiple-sheets-in-excel",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#extracting-data-from-multiple-sheets-in-excel",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "3. Extracting data from multiple sheets in excel",
    "text": "3. Extracting data from multiple sheets in excel\nThere are times where the data in excel might come from multiple sheets and we need to read in the data from all the sheets. The most straightforward way which I have found is using pd.read_excel with the parameter sheet_name=None. This will read all of the sheets in excel into a dictionary with the sheet name as the key and the data in DataFrames tagged to each sheet name.\nHere we use an example where a sample SAP data dictionary with the relevant data schema. The details for each table are shared in each sheet while each sheet in excel represents the respective tables.\n\n\n\nSample SAP Data Dictionary shared via Excel\n\n\nWe can read in the data in all the sheets with the parameter sheet_name set to None. Here instead of a single data frame, we will have a dictionary with each sheet name as key and the data frame in the sheet as the corresponding value of the dictionary.\n# Reading in the data\nsample_SAP_data_dict = \"./data/SAP Data Dictionary Selected Tables.xlsx\"\ndata_sheet_dict = pd.read_excel(sample_SAP_data_dict, sheet_name=None)\n# Display the names of each sheet in excel\nlist_sheets = list(data_sheet_dict.keys())\nprint(f\"There are {len(list_sheets)} sheets in the excel file\")\nprint(f\"The sheets are {list_sheets}\")\nAs we print out the keys, we can see the sheet names being listed. In our example, this corresponds with some of the SAP table names.\n\n\n\nDescription of excel file\n\n\nBelow is an example when the key “BSEG” is applied for the dictionary, we can get the corresponding data frame which was read from the sheet containing the schema of the table.\n\n\n\nSample Data Frame for BSEG table\n\n\nSubsequent analysis can be run at scale using the normal python dictionary and pandas functions and methods.\nOne of the analysis performed for our sample Data Dictionary is to find out which tables houses the similar data fields. We will first use a pivot_table method to create a new table where the column fields are shown as columns for each table. From there we join the tables together using pd.concat.\ndef extract_columns_info(df, sheet_name, list_column_field):\n    df_temp = df.copy()\n    df_temp[\"SAP_Table\"] = sheet_name\n    df_temp[\"Present\"] = 1\n    columns_select = [\"SAP_Table\"] + list_column_field + [\"Present\"]\n    \n    return df_temp[columns_select].pivot_table(\n        columns=list_column_field, values=\"Present\", index=[\"SAP_Table\"]\n    )\nlist_column_field = [\"Field\"]\nlist_df = []\nfor sheet_name, df in data_sheet_dict.items():\n    df_processed = extract_columns_info(df, sheet_name, list_column_field)\n    list_df.append(df_processed)\n\ndf_consolidated = pd.concat(list_df)\ndf_consolidated.fillna(\"\", inplace=True)\nIn the final result, we can we can see the common fields across various SAP tables. Below is a sample of the output.\n\n\n\nSample output of final table displaying common fields across different tables"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#reading-.txt-source-files",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#reading-.txt-source-files",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "4. Reading .txt source files",
    "text": "4. Reading .txt source files\nFor input data sources in .txt format, we can use the python open function directly to read the files. There will usually be text processing required depending on the delimiters used in the files.\nI wrote a seperate article for the cleaning of SAP .txt input file format. The article can be found here\nThe .read() method can be used to read in the entire data source. There are usually some processing involved e.g. all types of new line characters are represented with .\n# Converts any type of new line to \\n\nwith open(\"data/Sample SAP Format.txt\", encoding=\"utf-8\") as f:\n    content_raw = f.read()\n\n\n\nData with only new line character\n\n\nThere are times where the type of new line characters are important. In this case, we set the parameter newline= “” and we can see below that the original Carriage Return Line Feed () in the content.\n# Alternative way to read the file with CRLF (Carriage Return Line Feed intact)\nwith open(\"data/Sample SAP Format.txt\", encoding=\"utf-8\", newline=\"\") as f:\n    content_raw_2 = f.read()\n\n\n\nData Maintaining original carriage return linefeed\n\n\nAnother method common used is .readlines() which will break up each individual lines at the line breaks and provide the data in a list. An example is shown below. From the individual lines, the data can be further split based on established delimiters or other business rules.\nwith open(\"data/Sample SAP Format.txt\", encoding=\"utf-8\") as f:\n    content_split_lines = f.readlines()\n\n\n\nOutput list per line"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#writing-and-reading-parquet-files",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#writing-and-reading-parquet-files",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "5. Writing and reading Parquet Files",
    "text": "5. Writing and reading Parquet Files\nParquet is a type of file format for storing and organizing large amounts of data in a efficient and organized manner. The files are in the .parquet extension.\nIt is a columnar storage format, meaning that data is stored by columns instead of rows. This allows for faster data retrieval, especially for queries that only need to access a small subset of columns. The format is optimized for use with big data technologies like Apache Hadoop and Apache Spark and is widely used in data warehousing and big data analytics applications.\nFor setting up analytics data lakes parquet has the advantages of being smaller in size as it is compressed and being able to retain the data schema types compared to storing the data in csv. However, as the format is in a binary format, it will not be human readable.\nWriting of Data Frame into Parquet Files\nTo write a data frame into parquet files, we first need to install the pyarrow libarary.\npip install pyarrow\nAfter installation we can use pandas to_parquet method to store the data from a data frame into parquet format. Please see the example below on the code comparison between writing to a csv and parquet file and notice how much faster the writing data in parquet format is as compared to csv. Also, we can see the difference in file size of the parquet file vs csv.\n#Writing to CSV\ndf_retail_original.to_csv(\"./data/Online_Retail.csv\", index=False)\n#Writing to Parquet\ndf_retail_original.to_parquet(\"./data/Online_Retail.parquet\")\n\n\n\nTime taken to write data to parquet vs to csv\n\n\n\n\n\nDifference in size between CSV and parquet\n\n\nReading of Data Frame from parquet files\nWe can use pandas read_parquet method with pyarrow engine to read data from existing parquet file. Using the same method, we read the parquet and csv files previously written.\n#Reading from CSV\ndf_from_csv = pd.read_csv(\"./data/Online_Retail.csv\")\n#Reading from Parquet\ndf_from_parquet = pd.read_parquet(\"./data/Online_Retail.parquet\", engine=\"pyarrow\")\n\n\n\nTime taken to load data from parquet vs csv files\n\n\nThanks for reading and hope this was useful in some way!"
  },
  {
    "objectID": "posts/2022-08-12-python-io-diff-sources/index.html#references",
    "href": "posts/2022-08-12-python-io-diff-sources/index.html#references",
    "title": "Handle various input data source formats (csv, xlsx, xlsb, txt, parquet) with Python and Pandas",
    "section": "References",
    "text": "References\n\nhttps://www.kaggle.com/code/paultimothymooney/how-to-resolve-a-unicodedecodeerror-for-a-csv-file/notebook\nhttps://www.the-analytics.club/pandas-excel-multiple-sheets"
  },
  {
    "objectID": "posts/2022-10-06-Donut-Doc-Understanding/index.html",
    "href": "posts/2022-10-06-Donut-Doc-Understanding/index.html",
    "title": "Impressive yet easy to implement Document Understanding system with OCR-free Donut Transformers Model in Python",
    "section": "",
    "text": "Image Generated from Stable Diffusion with the prompt “A colorful donut speaking to a document”"
  },
  {
    "objectID": "posts/2022-10-06-Donut-Doc-Understanding/index.html#introduction",
    "href": "posts/2022-10-06-Donut-Doc-Understanding/index.html#introduction",
    "title": "Impressive yet easy to implement Document Understanding system with OCR-free Donut Transformers Model in Python",
    "section": "Introduction",
    "text": "Introduction\nRecently, with the release of the stable version of transformers v4.22 in September 2022, there were some new interesting models which were released. The Donut Transformers model, an OCR-Free Visual Document Understanding (VDU), was particularly interesting as I was looking for a image to text data extraction model to deploy in one of the current projects. More details about the model can be found here: OCR-free Document Understanding Transformer by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\nOverall, I found the version fine-tuned on DocVQA (visual question answering on documents) to be the most versatile and easy to implement and use.\n\nText extracted is highly accurate based on the pretrained model and it can even recognize hand written digits\nThe same data can be extracted from images of different file formats and structure as long as the identifying segment e.g. total, balance etc. is present\nData to be extracted from images can be customized based on the questions being fed to the model\nIt is a very magical and amazing experience to interact with scanned copies of receipts, invoices with questions =)"
  },
  {
    "objectID": "posts/2022-10-06-Donut-Doc-Understanding/index.html#optimizing-on-a-typical-document-understanding-pipeline",
    "href": "posts/2022-10-06-Donut-Doc-Understanding/index.html#optimizing-on-a-typical-document-understanding-pipeline",
    "title": "Impressive yet easy to implement Document Understanding system with OCR-free Donut Transformers Model in Python",
    "section": "OPTIMIZING ON A TYPICAL DOCUMENT UNDERSTANDING PIPELINE",
    "text": "OPTIMIZING ON A TYPICAL DOCUMENT UNDERSTANDING PIPELINE\n\n\n\nA typical Document Understanding Process as illustrated by UiPath (https://www.uipath.com/blog/product-and-updates/introducing-document-understanding-process-documents-intelligently)\n\n\nThe typical document understanding work flow will be very similar as the process flow illustrated above where the first step would using OCR (Optical Character Recognition) library or software to extract the raw text from the document. Common popular libraries for implementing OCR include pytesseract and tesserocr. Based on the raw text extracted, additional Machine Learning Language Modeling or Regex Text Extraction will need to be employed to retrieve useful information from the raw text.\nHowever, with the Donut DocVQA model, this end to end process can be greatly simplified and optimized."
  },
  {
    "objectID": "posts/2022-10-06-Donut-Doc-Understanding/index.html#installation",
    "href": "posts/2022-10-06-Donut-Doc-Understanding/index.html#installation",
    "title": "Impressive yet easy to implement Document Understanding system with OCR-free Donut Transformers Model in Python",
    "section": "INSTALLATION",
    "text": "INSTALLATION\nThere are two main libraries required to implement the Donut VQA model:\n\nPyTorch (https://pytorch.org/TensorRT/tutorials/installation.html)\nTransformers and Sentence Piece\n\npip install transformers[sentencepiece]"
  },
  {
    "objectID": "posts/2022-10-06-Donut-Doc-Understanding/index.html#sample-notebook",
    "href": "posts/2022-10-06-Donut-Doc-Understanding/index.html#sample-notebook",
    "title": "Impressive yet easy to implement Document Understanding system with OCR-free Donut Transformers Model in Python",
    "section": "SAMPLE NOTEBOOK",
    "text": "SAMPLE NOTEBOOK\nThe Donut related documentation on Hugging Face (https://huggingface.co/docs/transformers/main/en/model_doc/donut) and the tutorial by Niels Rogge (https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Donut/DocVQA) provided the complete starter code for using the transformers model.\nReferencing from the materials, I adapted the code to apply the transformer model to work with multiple images within a specified folder. A sample of my code can be found at:\nhttps://github.com/ZS-Weng/Machine_Learning/DonutDocVQA\nThe core of the code function is as below:\n\nfrom transformers import DonutProcessor, VisionEncoderDecoderModel\n\nprocessor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\nmodel = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n\ndef get_answer(folder_path, filename, question, model, processor):\n\n    image = Image.open(folder_path/filename).convert('RGB')\n\n    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n    \n    prompt = f\"<s_docvqa><s_question>{question}</s_question><s_answer>\"\n    decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n\n    outputs = model.generate(pixel_values.to(device),\n                               decoder_input_ids=decoder_input_ids.to(device),\n                               max_length=model.decoder.config.max_position_embeddings,\n                               early_stopping=True,\n                               pad_token_id=processor.tokenizer.pad_token_id,\n                               eos_token_id=processor.tokenizer.eos_token_id,\n                               use_cache=True,\n                               num_beams=1,\n                               bad_words_ids=[[processor.tokenizer.unk_token_id]],\n                               return_dict_in_generate=True,\n                               output_scores=True)\n    \n    seq = processor.batch_decode(outputs.sequences)[0]\n    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n    \n    return processor.token2json(seq)"
  },
  {
    "objectID": "posts/2022-10-06-Donut-Doc-Understanding/index.html#results",
    "href": "posts/2022-10-06-Donut-Doc-Understanding/index.html#results",
    "title": "Impressive yet easy to implement Document Understanding system with OCR-free Donut Transformers Model in Python",
    "section": "RESULTS",
    "text": "RESULTS\n\n1. Testing on a general receipt\nDonutDocVQA was able to pick up both the title and the total amount on a general receipt.\n\n\n\nDonutDocVQA information extraction on a generic receipt\n\n\n\n\n2. Testing on a receipt with hand written details\nDonutDocVQA was able to perform well to detect even information based on hand written digits.\n\n\n\nDonutDocVQA information extraction on a receipt with hand written information\n\n\n\n\n3. Testing on a F1 Poster\nEven though the digits were not that clear on a red background and the price of the categories “Pit Combination” is not right beside the category text, DonutDocVQA was able to pick up the correct prices.\n\n\n\nDonutDocVQA information extraction on Singapore F1 2022 poster\n\n\n\n\n4. Extracting Similar Information from Multiple Documents\nFor multiple documents with different formats as show below where there is a mix of different receipts, invoices, etc. a loop can be setup to extract similar information and consolidated into a tabular data structure.\n\n\n\nInput documents fed into DocVQA\n\n\n\n\n\nOutput results from the documents above"
  },
  {
    "objectID": "posts/2022-10-06-Donut-Doc-Understanding/index.html#conclusion",
    "href": "posts/2022-10-06-Donut-Doc-Understanding/index.html#conclusion",
    "title": "Impressive yet easy to implement Document Understanding system with OCR-free Donut Transformers Model in Python",
    "section": "CONCLUSION",
    "text": "CONCLUSION\nThe Donut Transformers model is a versatile and impressive tool for document understanding which can be used out of the box leveraging on the pre-trained weights. I feel that this could be a game changer which can potentially save a significant amount of time and resources for a variety of document understanding tasks expediting the end to end development process. Lastly, the intuitive way of using questions to query images would enhance user experience and drive more collaborations and interesting ideas in applying technology in novel ways.\nThanks for reading and hope the information was useful in some way!"
  },
  {
    "objectID": "posts/2022-10-18-Regex-n-Python/index.html",
    "href": "posts/2022-10-18-Regex-n-Python/index.html",
    "title": "Applications of Regex and Python in data transformation for masking of sensitive information and extraction of date details from free text",
    "section": "",
    "text": "Image generated from Stability AI Stable Diffusion with prompt “colourful artistic patterns made up of text”"
  },
  {
    "objectID": "posts/2022-10-18-Regex-n-Python/index.html#introduction",
    "href": "posts/2022-10-18-Regex-n-Python/index.html#introduction",
    "title": "Applications of Regex and Python in data transformation for masking of sensitive information and extraction of date details from free text",
    "section": "Introduction",
    "text": "Introduction\nThere are many useful applications of Regex. In this article, I would like to cover two of them commonly used for my projects in Singapore. They are\n\nMasking of sensitive Singapore National ID information\nExtraction of relevant date details from a text field\n\nThe full set of codes in a Jupyter Notebook format can be found in the following link https://github.com/ZS-Weng/Data_Engineering/tree/main/Regex\nThe full code for the various functions can also be found at the end of the article."
  },
  {
    "objectID": "posts/2022-10-18-Regex-n-Python/index.html#masking-of-singapore-national-id-information",
    "href": "posts/2022-10-18-Regex-n-Python/index.html#masking-of-singapore-national-id-information",
    "title": "Applications of Regex and Python in data transformation for masking of sensitive information and extraction of date details from free text",
    "section": "1. Masking of Singapore National ID information",
    "text": "1. Masking of Singapore National ID information\nIn Singapore, the National ID starts with one of the alphabets, S,T,F or G followed by 7 digits and ends with another alphabet e.g. S1234567A. With a simple Regex function, we can detect the occurrence of text matching the pattern and mask the information automatically to a format such as SXXXX567A.\nimport re\ndef mask_nric(text):\n    nric_regex = re.compile('([STFG])\\d{4}(\\d{3}[A-Z])')\n    return nric_regex.sub(r'\\1XXXX\\2', text)\nBelow is the example of the original text and processed text where the sensitive information have been masked:\n\n\n\nMasking of National ID details within Text"
  },
  {
    "objectID": "posts/2022-10-18-Regex-n-Python/index.html#extraction-of-date-details-from-service-text-field",
    "href": "posts/2022-10-18-Regex-n-Python/index.html#extraction-of-date-details-from-service-text-field",
    "title": "Applications of Regex and Python in data transformation for masking of sensitive information and extraction of date details from free text",
    "section": "2. Extraction of date details from Service Text Field",
    "text": "2. Extraction of date details from Service Text Field\nThere are several formats which can represent date and time and I will be highlighting the common scenarios for date representation. The code can be customized to the different date formats accordingly.\n\n2a. Extraction of date details where month is represented by abbreviated or long month name\nIn this section, the date pattern to be extracted is where the month is an abbreviated or full month name e.g. 22 Dec 2021, 18 October 22 etc. The Regex Pattern for matching the date and output with some additional processing are shown below.\nstring_date_pattern = re.compile(r'''(\\d{1,2})?[\\s-]?((?:(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)))[\\w]?['\\s-]?['\\s-]?(20\\d{2}|\\d{2})(?:\\s|[A-Za-z]|[\\(\\)\\.\\]]|[-_/]|$)''', re.VERBOSE | re.IGNORECASE)\n\n\n\nOutput for Date Pattern Extraction based on Abbreviated and Long Month Names\n\n\n\n\n2b. Extraction of date details where the date is represented in numeric format\nDates are also commonly represented entire in numbers and there are various formats where the date can start with the year, month or day e.g. YYYY-MM-DD, MM.DD.YY, DD/MM/YY etc. For the example, I am using a day-first date pattern and the Regex code and sample output are as below.\nday_first_pattern = re.compile(r'''([0-3]?[0-9])[./]([01]?[0-9])[./](20\\d{2}|\\d{2})(?:\\s|[A-Za-z]|[\\(\\)\\.\\]]|[-_]|$)''', re.VERBOSE | re.IGNORECASE)\n\n\n\nOutput for Date Pattern Extraction based on Day First Numeric Date Pattern\n\n\n\n\n2c. Extraction of date details where month is represented by short and long spelling\nThere are other instances where date are represented in a more abstract level, in this example by Quarter and Year. In this case, the first date of the quarter is used to represent the date extracted from the Quarter Year format. The Regex code and sample output are as shown below.\nq_year_pattern = re.compile(r'''Q([1-4])[\\s-]?(20\\d{2}|\\d{2})''', re.VERBOSE | re.IGNORECASE)\n\n\n\nOutput for Date Pattern Extraction based on Quarter Year Pattern\n\n\nThanks for reading and hope the information was useful in some way!"
  },
  {
    "objectID": "posts/2022-10-18-Regex-n-Python/index.html#full-codes-including-regex-pattern-and-functions",
    "href": "posts/2022-10-18-Regex-n-Python/index.html#full-codes-including-regex-pattern-and-functions",
    "title": "Applications of Regex and Python in data transformation for masking of sensitive information and extraction of date details from free text",
    "section": "Full Codes including Regex Pattern and Functions",
    "text": "Full Codes including Regex Pattern and Functions\n\n### String Date Pattern Regex\n\nstring_date_pattern = re.compile(r'''\n(\\d{1,2})?\n[\\s-]?\n((?:(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)))\n[\\w]?\n['\\s-]?\n['\\s-]?\n(20\\d{2}|\\d{2})\n(?:\\s|[A-Za-z]|[\\(\\)\\.\\]]|[-_/]|$)\n''', re.VERBOSE | re.IGNORECASE)\n\n### Function to apply String Date Pattern Regex\n\ndef check_string_date(compiled_pattern, text):\n\n    # List to contain output date format \n    list_output_date = []\n\n    # Use text matching to match details \n    list_dates = compiled_pattern.findall(text)\n    \n    for record in list_dates:\n        day, month, year = record\n        flag_blank_day = (len(day)==0)\n        #Putting the default date to 1 if only Month and Year details are present\n        if flag_blank_day:\n            day = \"01\"\n        if len(year) == 2:\n            year = '20' + year\n        month = month.capitalize()\n\n        str_date = ' '.join([day,month,year])\n        #Handle Scenario where the month is is 3 Letter Short Form \n        if len(month) == 3:\n            try:\n                list_output_date.append(datetime.strptime(str_date, \"%d %b %Y\"))\n            #In the event that the date is keyed in out of range \n            except ValueError:\n                str_date = ' '.join([\"01\",month,year])\n                list_output_date.append(datetime.strptime(str_date, \"%d %b %Y\"))\n\n        #Handle Scenario where the month is is in long Form \n        else:\n            try:\n                list_output_date.append(datetime.strptime(str_date, \"%d %B %Y\"))\n            #In the event that the date is keyed in out of range \n            except ValueError:\n                str_date = ' '.join([\"01\",month,year])\n                list_output_date.append(datetime.strptime(str_date, \"%d %B %Y\"))\n\n    if len(list_output_date) > 0:\n        return (list_output_date)\n    else:\n        return [pd.NaT]\n       \n### Numeric Date Pattern Regex\n\nday_first_pattern = re.compile(r'''\n([0-3]?[0-9])\n[./]\n([01]?[0-9])\n[./]\n(20\\d{2}|\\d{2})\n(?:\\s|[A-Za-z]|[\\(\\)\\.\\]]|[-_]|$)\n''', re.VERBOSE | re.IGNORECASE)\n\n### Function to apply Numeric Date Pattern Regex\n\ndef check_numeric_date(compiled_pattern, text, match_type='day_first'):\n\n    # List to contain output date format \n    list_output_date = []\n\n     # Use text matching to match details \n    list_dates = compiled_pattern.findall(text)\n\n    for record in list_dates:\n        if match_type == 'day_first':\n            day, month, year = record\n            day = int(day)\n            month = int(month)\n            if len(year) == 2:\n                year = int('20' + year)\n            else:\n                year = int(year)\n\n            try:\n                list_output_date.append(datetime(year,month,day))\n            #In the event that the date is keyed in out of range \n            except ValueError:\n                print(f\"Invalid date: {record}\")\n\n        if match_type == 'month_first':\n            day, month, year = record\n            day = int(day)\n            month = int(month)\n            if len(year) == 2:\n                year = int('20' + year)\n            else:\n                year = int(year)\n\n            try:\n                list_output_date.append(datetime(year,month,day))\n            #In the event that the date is keyed in out of range \n            except ValueError:\n                print(f\"Invalid date: {record}\")\n        \n    if len(list_output_date) > 0:\n        return (list_output_date)\n    else:\n        return [pd.NaT]\n\n### Quarter Year Date Pattern Regex\n\nq_year_pattern = re.compile(r'''\nQ\n([1-4])\n[\\s-]?\n(20\\d{2}|\\d{2})\n''', re.VERBOSE | re.IGNORECASE)\n\n### Function to apply Quarter Year Date Pattern Regex\n\ndef check_quarter_year(compiled_pattern, text):\n\n    # List to contain output date format \n    list_output_date = []\n\n    # Use text matching to match details \n    list_dates = compiled_pattern.findall(text)\n\n    for record in list_dates:\n\n        quarter, year = record\n        quarter = int(quarter)\n        #Get the starting month of the quarter\n        month = 1 + (quarter-1) * 3\n        if len(year) == 2:\n            year = int('20' + year)\n        else:\n            year = int(year)\n        \n        list_output_date.append(datetime(year,month,1))\n\n    if len(list_output_date) > 0:\n        return (list_output_date)\n    else:\n        return [pd.NaT]"
  },
  {
    "objectID": "posts/2022-10-18-Regex-n-Python/index.html#recommended-resource-to-learn-more-about-regex",
    "href": "posts/2022-10-18-Regex-n-Python/index.html#recommended-resource-to-learn-more-about-regex",
    "title": "Applications of Regex and Python in data transformation for masking of sensitive information and extraction of date details from free text",
    "section": "Recommended Resource to learn more about Regex",
    "text": "Recommended Resource to learn more about Regex\nOut of the different resources I have used to learn about Regex, I found the materials from Al Sweigart most engaging. You can find the free chapter covering Regex from his book here: https://automatetheboringstuff.com/2e/chapter7/"
  },
  {
    "objectID": "posts/2023-02-03-hierarchical-clustering/index.html",
    "href": "posts/2023-02-03-hierarchical-clustering/index.html",
    "title": "Hierarchical Clustering can be more suitable compared to KMeans when grouping customers based on purchase behaviors and detecting outliers",
    "section": "",
    "text": "Image generated from DALL E with prompt artistic clusters and outliers"
  },
  {
    "objectID": "posts/2023-02-03-hierarchical-clustering/index.html#introduction",
    "href": "posts/2023-02-03-hierarchical-clustering/index.html#introduction",
    "title": "Hierarchical Clustering can be more suitable compared to KMeans when grouping customers based on purchase behaviors and detecting outliers",
    "section": "Introduction",
    "text": "Introduction\nClustering can be a particularly useful starting point to embark on a Machine Learning journey especially when data labels are yet to be built up. One of the easy way to start is to employ clustering based on the purchase behaviors of customers or departments. This can be done with with any datasets which contains sales data based on product categories. Based on the clustering results, customers can be grouped into meaningful clusters for further analysis. In addition, outliers can be detected for clusters with few customers within.\nKMeans and Hierarchical Clustering are two of the most common and popular techniques used today. In this article, I would like to examine in more details on the end to end process from data preprocessing to post processing of clustering results and highlight some of the differences and usefulness of the two techniques based on my experiences deploying them on projects.\nLinks to code and data\n\nThe sample dataset used for illustration can be downloaded from Kaggle https://www.kaggle.com/datasets/vivek468/superstore-dataset-final\nThe full codes in notebook and the dataset can be found from the following GitHub link: https://github.com/ZS-Weng/Machine_Learning/tree/main/Clustering"
  },
  {
    "objectID": "posts/2023-02-03-hierarchical-clustering/index.html#standard-data-preprocessing",
    "href": "posts/2023-02-03-hierarchical-clustering/index.html#standard-data-preprocessing",
    "title": "Hierarchical Clustering can be more suitable compared to KMeans when grouping customers based on purchase behaviors and detecting outliers",
    "section": "Standard Data Preprocessing",
    "text": "Standard Data Preprocessing\nThe dataset contains 9994 rows and 21 columns. The overview of the dataset is as shown below:\n\n\n\nOverview of the Store Dataset\n\n\nTo perform the initial clustering analysis, we will only be making use of the Customer, Sub-Category and Sales Categories. We will first transform the data by performing a pandas pivot_table operation so that we can get the amount spent by each customers on the sub categories.\ndf_key_fields = df_raw[[\"Customer ID\", \"Sub-Category\", \"Sales\"]].copy()\ndf_cust_sales = df_key_fields.pivot_table(\n    values=\"Sales\", columns=\"Sub-Category\", aggfunc=np.sum, index=\"Customer ID\"\n).reset_index()\nAfter the pivot_table operation, a sample of the output is as shown below:\n\n\n\nSample of data showing the total spend on each sub category based on each customer\n\n\nThere are many instances where there might be some customers who spend and unproportionate larger amount compared to most of the other customers, causing the results to skew. To prevent that, we will normally apply a log transformation to the sales amount to stabilize the variance of the variable and make the relationship between the variable and the target variable more linear.\ndf_cust_sales.iloc[:, 1:] = df_cust_sales.iloc[:, 1:].applymap(lambda x: np.log(x + 1))"
  },
  {
    "objectID": "posts/2023-02-03-hierarchical-clustering/index.html#pca-principal-component-analysis-preprocessing-before-kmeans-clustering",
    "href": "posts/2023-02-03-hierarchical-clustering/index.html#pca-principal-component-analysis-preprocessing-before-kmeans-clustering",
    "title": "Hierarchical Clustering can be more suitable compared to KMeans when grouping customers based on purchase behaviors and detecting outliers",
    "section": "PCA (Principal Component Analysis) preprocessing before KMeans Clustering",
    "text": "PCA (Principal Component Analysis) preprocessing before KMeans Clustering\nIt is usually best practice to perform PCA (Principal Component Analysis) before KMeans clustering as PCA could help to reduce dimensions and noise and remove correlated features to help the algorithm.\nFor PCA, the number of components will need to be defined and we usually would choose the number of components based on a a percentage of explained variance e.g. 90%.\nFor visualization, we can run pca based on the range of n_components and we can visualize how the amount of variability in the dataset is increasingly covered with larger n_components set.\npca = PCA()\ndf_pca = pca.fit_transform(df_for_pca)\nexp_var = np.cumsum(pca.explained_variance_ratio_)\nplt.bar(range(1, df_pca.shape[1] + 1), pca.explained_variance_ratio_, align=\"center\")\nplt.step(\n    range(1, df_pca.shape[1] + 1), np.cumsum(pca.explained_variance_ratio_), where=\"mid\"\n)\nplt.ylabel(\"Explained variance ratio\")\nplt.xlabel(\"Principal Components\")\nplt.show()\n\n\n\nPlot on the amount of variability covered against number of components selected for PCA\n\n\nTo directly get the number of components based on the ratio threshold, we can apply a mask to mask values smaller than the threshold and find the location of the smallest value using argmin.\nexp_var = np.ma.MaskedArray(exp_var, exp_var < 0.9)\nn_components = np.argmin(exp_var)"
  },
  {
    "objectID": "posts/2023-02-03-hierarchical-clustering/index.html#kmeans-clustering",
    "href": "posts/2023-02-03-hierarchical-clustering/index.html#kmeans-clustering",
    "title": "Hierarchical Clustering can be more suitable compared to KMeans when grouping customers based on purchase behaviors and detecting outliers",
    "section": "KMeans Clustering",
    "text": "KMeans Clustering\nThe first method to explore would be KMeans clustering where the number of clusters need to be predetermined. The common way to find the optimal number of clusters would be to iterate through multiple clusters and find the optimal number of clusters where the distortion decreases the least with an increase in K.\n\n\n\nImage of selection of the optimal number of clusters K using the Elbow method\n\n\nIn many of the real life datasets, the plot of distortion against number of clusters, K might not be so visually straightforward. Below is a sample code to generate the plot and the results based on the superstore dataset.\ndistortions = []\nfor i in range(1, 30):\n    km = KMeans(n_clusters=i, init=\"k-means++\", max_iter=300, random_state=0)\n    km.fit(pca_arr)\n    distortions.append(km.inertia_)\n\nprint(len(distortions))\nplt.plot(range(1, len(distortions) + 1), distortions, marker=\"o\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Distortion\")\nplt.tight_layout()\nplt.show()\nFrom the graph, we can see that there is not a very straightforward point seen as the elbow point.\n\n\n\nDistortion plot in real life when there can be many clusters\n\n\nAnother way to find the optimal number of clusters, k for KMeans is calculating the average silhouette score for different number of k which can be achieved based on the code below.\nMore details on sihouette score and plots can be found from the scikit learn code example page: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\nfrom sklearn.metrics import silhouette_score\n\nlist_i = []\nlist_score = []\nfor i in range(2, n_clusters):\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    km = KMeans(n_clusters=i, init=\"k-means++\", max_iter=500, random_state=0)\n    cluster_labels = km.fit_predict(pca_arr)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(pca_arr, cluster_labels)\n    list_i.append(i)\n    list_score.append(silhouette_avg)\nFrom the graph, generated we can see the the optimal number of clusters with the highest score will be 2 and 4. In many cases, such small number of clusters might not provide useful and meaningful grouping and larger cluster values might also not be so straightforward to be selected.\n\n\n\nPlot of Avg Silhouette Score against number of clusters"
  },
  {
    "objectID": "posts/2023-02-03-hierarchical-clustering/index.html#hierarchical-clustering",
    "href": "posts/2023-02-03-hierarchical-clustering/index.html#hierarchical-clustering",
    "title": "Hierarchical Clustering can be more suitable compared to KMeans when grouping customers based on purchase behaviors and detecting outliers",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nThere are instances where we found Hierarchical clustering to be more effective. Below are some of the hierarchical clustering packages in Scikit Learn.\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\nHierarchical clustering is akin more to a bottom up approach where records are clustered together based on the distance away from each other. Eventually, all customers will be merged into a single cluster. We can see the details of how the cluster forms with each merge by using linkage.\nrow_clusters = linkage(df_for_pca.values, method=\"ward\", metric=\"euclidean\")\ndf_hc = pd.DataFrame(\n    row_clusters,\n    columns=[\"row label 1\", \"row label 2\", \"distance\", \"no. of items in clust.\"],\n    index=[f\"Merge {(i + 1)}\" for i in range(row_clusters.shape[0])],\n)\n\n\n\nTable showing cluster components based on merges\n\n\nFrom there we can determine the number of merges e.g. 85% on the data and find the corresponding distance metrics which we can use to apply clustering. From experience, a lower distance threshold can be better used to find outliers while a higer threshold can help to achieve larger clusters for analysis or further actions.\n# Finding the merge based on 85% of merges\nmerge_threshold = 0.85\nrow = int(merge_threshold * df_hc.shape[0])\ndistance_threshold = df_hc.iloc[row, 2]\nprint(row, distance_threshold)\nFrom the distance threshold obtained, we can visualize how customers are grouped together using dendrogram.\nlabels = df_cust_sales.iloc[:, 0].values\nplt.figure(figsize=(300, 12))\nrow_dendr = dendrogram(row_clusters, labels=labels, color_threshold=distance_threshold)\nplt.ylabel(\"Euclidean distance\")\nplt.xlabel(\"Customer ID\")\nplt.show()\nIn the dendrogram generated below, different colors represent different clusters and the x-axis labels are the customer ids from the dataset.\n\n\n\nGrouping of Customers\n\n\nOnce the result looks satisfactory, the clusters can be generated using AgglomerativeClustering.\n# Use Agglomerative Clustering based on Threshold\nfrom sklearn.cluster import AgglomerativeClustering\n\nac = AgglomerativeClustering(\n    n_clusters=None,\n    distance_threshold=distance_threshold,\n    affinity=\"euclidean\",\n    linkage=\"ward\",\n)\n\nhc_cluster = ac.fit_predict(df_for_pca.values)"
  },
  {
    "objectID": "posts/2023-02-03-hierarchical-clustering/index.html#data-post-processing-and-application-of-clustering-information",
    "href": "posts/2023-02-03-hierarchical-clustering/index.html#data-post-processing-and-application-of-clustering-information",
    "title": "Hierarchical Clustering can be more suitable compared to KMeans when grouping customers based on purchase behaviors and detecting outliers",
    "section": "Data Post Processing and Application of Clustering Information",
    "text": "Data Post Processing and Application of Clustering Information\nWhen the cluster details are generated, the cluster number is usually random in nature and it might take a lot of effort to provide a meaningful description for each cluster. Some post processing of the data which are found to be helpful include extracting details based on the attributes of the clusters (sub category in this example), sort the clusters based on a metric like total sales or number of customers and assign a cluster/group number based on the rank to provide a more meaningful cluster/group number. The aggregation for a cluster can be easily achieved with groupby.\ndf_cluster_cat_count = (\n    df_cluster_original_amount.groupby([\"Cluster\", \"Sub-Category\"])\n    .agg({\"Sales\": \"sum\", \"Customer ID\": \"nunique\"})\n    .reset_index()\n)\ndf_cluster_cat_count.columns = [\n    \"Cluster\",\n    \"Sub-Category\",\n    \"Sales\",\n    \"Count of Customers in Cluster SubCat\",\n]\n\n\n\nAdditional Information to enhance Cluster Information"
  },
  {
    "objectID": "posts/2023-02-03-hierarchical-clustering/index.html#conclusion",
    "href": "posts/2023-02-03-hierarchical-clustering/index.html#conclusion",
    "title": "Hierarchical Clustering can be more suitable compared to KMeans when grouping customers based on purchase behaviors and detecting outliers",
    "section": "Conclusion",
    "text": "Conclusion\nFrom my experience, clustering can be a very useful in many situations especially as an unsupervised learning ML method when there is a lack of data labels for supervised learning. For transactional data, one of the first steps which can be taken to apply clustering is to group customers by their purchasing behavior on product categories.\nHierarchical Clustering can be a useful and flexible method to cluster the customers to find both outliers and generate groups of customers for follow up actions. There can also be standardization based on the percentage of merge to form the clusters.\nThe cluster id assigned from clustering algorithms will change between each runs and does not provide useful information. Data Post Processing through aggregation of relevant metrics and description can be useful to add more useful information to describe the clusters.\nThank you for reading and hope the information was useful in some way!"
  }
]