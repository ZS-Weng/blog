[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaishan Weng",
    "section": "",
    "text": "I am techno-functional consultant and solutions architect with over 14 years of experience leading strategic digital transformation, data analytics and data science initiatives in the retail, government, health & security and restaurant industries. A dynamic & passionate professional who specializes in driving innovation, delivering large and complex projects, and advising senior executives on leveraging technology to drive business outcomes. A data scientist/engineer with proven track record in deploying impactful and productive AI products.\nI am passionate about the impact and difference technology has made and strive to explore ways of making existing work simpler and easier so that we can have the time to focus on bigger visions."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Zaishan’s Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nQuick Start Guide on incorporating design thinking artifacts for requirements gathering during Project Initiation Phase for Agile Data Analytics Projects\n\n\n\n\n\n\n\nDesign Thinking\n\n\nRequirements Gathering\n\n\nData Analytics\n\n\nStakeholder Engagement\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\n  \n\n\n\n\nData cleaning on SAP data extracts in .txt format with Regex and Python\n\n\n\n\n\n\n\nData Engineering\n\n\nSAP\n\n\nPython\n\n\nRegex\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2022\n\n\nZaishan Weng\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "",
    "text": "During one of our recent projects involving the procure to pay process, our team encountered SAP raw data extracted from the system in .txt format which proved to be difficult to clean using traditional methods like readlines() or split() by delimiters due to some inherent data inconsistencies.\nRegex matching proved to be helpful for such scenarios to clean the data."
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#common-sap-tables-in-procure-to-pay-process",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#common-sap-tables-in-procure-to-pay-process",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Common SAP tables in Procure to Pay process",
    "text": "Common SAP tables in Procure to Pay process\nFirst, lets go through what are common data tables extracted from SAP as part of the Procure to Pay process.\nAccounting Related Tables\n\nBKPF: Accounting Document Header\nBSAK: Accounting: Secondary Index for Vendors\nBSEG: Accounting Document Segment\n\nPurchase Order related Tables\n\nEKKO: Purchasing Document Header\nEKPO: Purchasing Document Item\nEKBE: History per Purchasing Document\nEKKN: Account Assignment in Purchasing Document\n\nMaterial Tables\n\nMAKT: Material Descriptions\n\nThere are various websites which provide additional information about the SAP tables.\n\nhttps://www.se80.co.uk/training-education/sap-tables/\nhttps://www.tcodesearch.com/sap-tables/detail?id=BSEG (Might need to enter through changing the id in URL without requiring premium membership)\n\n\n\n\nExample of Data Dictionary for BSEG Table Source: https://www.se80.co.uk/"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#format-of-sap-data-extract-in-.txt-file",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#format-of-sap-data-extract-in-.txt-file",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Format of SAP data extract in .txt file",
    "text": "Format of SAP data extract in .txt file\nFor our project, the output SAP data extracts is in a .txt format and with the typical structure as shown below:\n\nThe column header details starts at line 4\nThe width of each column is consistent between the column headers and the data for each file extracted\nActual data content starts at line 5 till the end\n\n\n\n\nSample SAP TXT Data Extract Structure with Mock Data\n\n\nThe sample SAP data in txt format and Jupyter Notebook can be found on GitHub: https://github.com/ZS-Weng/Data_Engineering/tree/main/Data_Cleaning\nThe two major data discrepancies encountered are:\n\nNewline character inserted in some of the fields\nPipe (|) delimiters found within the actual data"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#full-code-and-output",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#full-code-and-output",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Full Code and Output",
    "text": "Full Code and Output\nThe full working code for the data cleaning is as shown below:\nimport pandas as pd\nimport re\n\n# Read File\nwith open(\"Sample SAP Format.txt\", encoding=\"utf-8\") as f:\n    content_raw = f.read()\n\n# Clean extra newline characters\nnew_line_pattern = re.compile(\"([^1|-])[\\n](.)|(.)[\\n]([^|-])\")\ncontent_cleaned_newline = new_line_pattern.sub(r\"\\1 \\2\", content_raw)\ncontent_split_line = content_cleaned_newline.split(\"\\n\")\n\n# Clean the rest of content\n\n# Extract Header and Row Pattern\nheader_string = content_split_line[3]\ncolumn_header = [token.strip() for token in header_string.split(\"|\")][1:-1]\nlist_column_width = [\n    \"(.{\" + str(len(column)) + \"})\" for column in header_string.split(\"|\")\n][1:-1]\ncolumn_string_pattern = \"[|]\" + \"[|]\".join(list_column_width) + \"[|]\"\n#Extract Data Body\ncolumn_pattern = re.compile(column_string_pattern)\ncleaned_content = [\n    [token.strip() for token in column_pattern.match(row).groups()]\n    for row in content_split_line[5:-2]\n]\ndf_clean = pd.DataFrame(cleaned_content, columns=column_header)\nFinal pandas data frame output:\n\n\n\nFinal pandas tabular output after data cleaning"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#detailed-walk-through-of-the-codes",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#detailed-walk-through-of-the-codes",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Detailed Walk-through of the codes",
    "text": "Detailed Walk-through of the codes\nCleaning of newline character prior to splitting lines\nIn order to clean the (newline) characters which are not valid, we first use the read() instead of readline() method as the readline() method will split the lines by the character automatically.\nwith open(\"Sample SAP Format.txt\", encoding=\"utf-8\") as f:        \n    content_raw = f.read()\nNext, we use the Regex pattern ([1|-])[\\n](.)|(.)[\\n]([^|-]) to find and the invalid characters. The pattern basically detects newline that do not start with the characters 1 , | or - and do not end with the characters | or -.\nFor the invalid newline characters found, we replace them with a space using the .sub() method.\nnew_line_pattern = re.compile(\"([^1|-])[\\n](.)|(.)[\\n([^|-])\")\ncontent_cleaned_newline = new_line_pattern.sub(r\"\\1 \\2\",content_raw)\ncontent_split_line = content_cleaned_newline.split(\"\\n\")\nOutput after cleaning and splitting lines:\n\n\n\nList of Data After the Line Split\n\n\nData Cleaning Steps for Individual Line Based on the above, we can see that the data we are interested in is the column header (4th line) and the rest of the data content (6th to 2nd last lines).\nWe will first split the column by the pipe “|” delimiter and getting the columns width to create the Regex matching string pattern.\nheader_string = content_split_line[3]\ncolumn_header = [token.strip() for token in header_string.split(\"|\")][1:-1]\nlist_column_width = [\"(.{\" + str(len(column)) + \"})\" for column in header_string.split(\"|\")][1:-1]\ncolumn_string_pattern = \"[|]\" + \"[|]\".join(list_column_width) + \"[|]\"\nThe column_string_pattern to match that is generated for the sample data will be as follows:\n[|](.{10})[|](.{4})[|](.{3})[|](.{10})[|](.{20})[|]\nThis column pattern matching pattern is dynamically generated and will be unique for each of the data extract file even for the same tables as the width is adjusted during data extract according to the content.\nWe will then use the matching pattern to extract the rest of the data (6th to 2nd last line) with re.match().groups() instead of using the str.split() method.\ncolumn_pattern = re.compile(column_string_pattern)\ncleaned_content = [[token.strip() for token in column_pattern.match(row).groups()] for row in content_split_line[5:-2]]\nThe output after content splitting:\n\n\n\nData in Nested List format after splitting of content of each individual line\n\n\nHere we can see that the extra delimiters e.g. ‘Item A|B|C’ does not affect the content splitting .\nFinally, we combine the cleaned header columns and data into a pandas data frame.\ndf_clean = pd.DataFrame(cleaned_content, columns=column_header)\nFinal pandas Data Frame output:\n\n\n\nFinal cleaned data output in pandas Data Frame"
  },
  {
    "objectID": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#useful-learning-resource-for-python-and-regex",
    "href": "posts/2022-06-25-Data-cleaning-on-SAP-data-extracts-in-txt-format-with-Regex-and-Python/index.html#useful-learning-resource-for-python-and-regex",
    "title": "Data cleaning on SAP data extracts in .txt format with Regex and Python",
    "section": "Useful Learning Resource for Python and Regex",
    "text": "Useful Learning Resource for Python and Regex\nWhen I was starting out, I found the book Automate the Boring Stuff with Python by Al Sweigart one of the best resources to learning about Python and Regex with many practical examples.\nThere is a free access option to the book on his website: https://automatetheboringstuff.com/ and this is the link to the specific chapter on Regex which formed the foundation on some of the implementation: https://automatetheboringstuff.com/2e/chapter7/"
  }
]